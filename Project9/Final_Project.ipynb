{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Laali.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_ukL4ZsXYlg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import required packages\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import random\n",
        "import numbers\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import randperm\n",
        "from torch._utils import _accumulate\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms, models\n",
        "import torchvision.transforms.functional as F\n",
        "import xml.etree.ElementTree as ET\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.nn as nn\n",
        "import time\n",
        "from skimage import io, transform\n",
        "import sklearn.metrics as skm\n",
        "import imutils\n",
        "from scipy.spatial import distance"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4NnKmsdf34P",
        "colab_type": "text"
      },
      "source": [
        "**Functions Required to prepare custom dataset**\n",
        "1. Finding bounding box details\n",
        "2. Heatmap generation\n",
        "3. Transformations applied to dataset\n",
        "4. Customized dataset preparation for both training and testing datasets\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32yTIKipX72U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_bnddetails(bndvalues):\n",
        "   \"\"\" Finding details of boundary box from xml tag.\n",
        "   Args:\n",
        "   bndvalues: boundary details of the bounding box.\n",
        "    \n",
        "   Returns:\n",
        "   corners of boundary box, center of the box, height and width as a array.\n",
        "   \"\"\"\n",
        "   xmin = int(bndvalues.find('xmin').text)\n",
        "   ymin = int(bndvalues.find('ymin').text)\n",
        "   xmax = int(bndvalues.find('xmax').text)\n",
        "   ymax = int(bndvalues.find('ymax').text)\n",
        "   return np.array((xmin,ymin,xmax,ymax,(xmin+xmax)/2,(ymin+ymax)/2,np.abs(xmax-xmin),np.abs(ymin-ymax))).reshape(-1,2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGVAGMgPOOqT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def New_bnddtls(bnddtls,scaling):\n",
        "    \"\"\" \n",
        "    Calculating the scaled boundary details\n",
        "    Args:\n",
        "    bnddtls: actual details of the boundary box\n",
        "    scaling: scaling factor to change the details of boundary box\n",
        "    Returns:\n",
        "    scaled boundary box details\n",
        "    \"\"\"\n",
        "    bnddtls[0] = bnddtls[0]/scaling\n",
        "    bnddtls[1] = bnddtls[1]/scaling\n",
        "    bnddtls[2][0] = (bnddtls[0, 0]+bnddtls[1, 0]) /2.0\n",
        "    bnddtls[2][1] = (bnddtls[0, 1]+bnddtls[1, 1])/2.0\n",
        "    bnddtls[3][0] = torch.abs(bnddtls[0, 0]-bnddtls[1, 0])\n",
        "    bnddtls[3][1] = torch.abs(bnddtls[0, 1]-bnddtls[1, 1])\n",
        "    return bnddtls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjGRLXyGOIcb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def HeatMap(bnddtls,filesize):\n",
        "    \"\"\" \n",
        "    Generating heatmap based on boundary details\n",
        "    Args:\n",
        "    bnddtls: actual details of the boundary box\n",
        "    filesize: size of the boundary box\n",
        "\n",
        "    Returns:\n",
        "    heat-map of size filesize/4 with center from bnddtls\n",
        "    \"\"\"\n",
        "    bnddtls.float()\n",
        "    height = bnddtls[3][0]\n",
        "    width  = bnddtls[3][1]\n",
        "    bnddtls = New_bnddtls(bnddtls,4.0)\n",
        "    img_heatmap = torch.zeros(int(filesize[0]/4),int(filesize[1]/4))\n",
        "    size=8\n",
        "    kernel = cv2.getGaussianKernel(size, 8)\n",
        "    kernel = np.dot(kernel, kernel.T)\n",
        "    kernel *= 100\n",
        "  \n",
        "    if bnddtls[2][1].item()+size > img_heatmap.shape[0]-1:\n",
        "                y_begin = img_heatmap.shape[0]-1-size\n",
        "    else:\n",
        "                y_begin = int(bnddtls[2][1].item())\n",
        "\n",
        "    if bnddtls[2][0].item()+size > img_heatmap.shape[1]-1:\n",
        "                x_begin = img_heatmap.shape[1]-1-size\n",
        "    else:\n",
        "                x_begin = int(bnddtls[2][0].item())\n",
        "\n",
        "    y_end = y_begin + (size)\n",
        "    x_end = x_begin + (size)\n",
        "    img_heatmap[y_begin : y_end, x_begin : x_end] = torch.from_numpy(kernel)\n",
        "    return img_heatmap"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SadZjqAEYjEL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ToTensor(object):\n",
        "    \"\"\"Convert ndarrays or PIL image in sample to Tensors.\"\"\"\n",
        "        # swap color axis because\n",
        "        # numpy image: H x W x C\n",
        "        # torch image: C X H X W\n",
        "\n",
        "    def __call__(self, sample):\n",
        "      items = dict()\n",
        "      for key in sample.keys():\n",
        "          if key == 'image':\n",
        "            image = sample[key]\n",
        "            image = F.to_tensor(image)\n",
        "            items[key] = image\n",
        "          else:\n",
        "            dtls = torch.FloatTensor(sample[key])\n",
        "            items[key] = dtls\n",
        "      return items"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OF__QQZLYq8j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Rescale(object):\n",
        "    \"\"\"\n",
        "    Rescale the object to the size of given in output_size\n",
        "    Args:\n",
        "    output_size (tensor): required size of the output image\n",
        "    \"\"\"\n",
        "    def __init__(self, output_size):\n",
        "        assert isinstance(output_size, (int, tuple))\n",
        "        self.output_size = output_size\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        items = dict()\n",
        "        image = sample['image']\n",
        "        w, h = image.size\n",
        "        if isinstance(self.output_size, int):\n",
        "           if h > w:\n",
        "              new_h, new_w = self.output_size * h / w, self.output_size\n",
        "           else:\n",
        "              new_h, new_w = self.output_size, self.output_size * w / h\n",
        "        else:\n",
        "           new_h, new_w = self.output_size\n",
        "\n",
        "        new_h, new_w = int(new_h), int(new_w)\n",
        "\n",
        "        for key in sample.keys():\n",
        "          if key == 'image':\n",
        "              img = F.resize(image, (new_h, new_w))\n",
        "              items[key] = img\n",
        "          else:\n",
        "              all_dtls = sample[key]\n",
        "              \n",
        "              for i, dtls in enumerate(all_dtls):\n",
        "                  all_dtls[i][0] = np.round(dtls[0] * np.array([new_w / w, new_h / h]), 0)\n",
        "                  all_dtls[i][1] = np.round(dtls[1] * np.array([new_w / w, new_h / h]), 0)\n",
        "                  all_dtls[i][3] = np.abs([dtls[0, 0]-dtls[1, 0], dtls[0, 1]-dtls[1, 1]])\n",
        "                  all_dtls[i][2] = np.array([dtls[0, 0]+dtls[3, 0]/2, dtls[0, 1]+dtls[3, 1]/2])\n",
        "\n",
        "              items[key] = all_dtls\n",
        "              \n",
        "        return items"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RP_cIf-KT4gV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RandomVerticalFlip(object):\n",
        "    \"\"\"\n",
        "    Vertical flip the given PIL Image randomly with a given probability.\n",
        "    Args:\n",
        "    p (float): probability of the image being flipped. Default value is 0.5\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, p=0.5):\n",
        "        self.p = p\n",
        "\n",
        "    def __call__(self, sample):\n",
        "      items = dict()\n",
        "      image = sample['image']\n",
        "      w, h = image.size\n",
        "      if random.random() < self.p:\n",
        "        for key in sample.keys():\n",
        "            if key == 'image':\n",
        "               image = F.vflip(image)\n",
        "               items[key] = image\n",
        "            else:\n",
        "              all_dtls = sample[key]\n",
        "              \n",
        "              for i, dtls in enumerate(all_dtls):\n",
        "                 if dtls[2][0] > 0 and dtls[2][1] > 0:\n",
        "                    dtls[0][1] = h-1-dtls[0][1]-dtls[3][1]\n",
        "                    dtls[1][1] = h-1-dtls[1][1]+dtls[3][1]\n",
        "                    dtls[2][1] = h-1-dtls[2][1]\n",
        "\n",
        "              items[key] = all_dtls\n",
        "      else:\n",
        "         return sample\n",
        "      return items"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1TW7Lq1fpuI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RandomHorizontalFlip(object):\n",
        "    \"\"\"\n",
        "    Horizontally flip the given PIL Image randomly with a given probability.\n",
        "\n",
        "    Args:\n",
        "    p (float): probability of the image being flipped. Default value is 0.5\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, p=0.5):\n",
        "        self.p = p\n",
        "\n",
        "    def __call__(self, sample):\n",
        "      items = dict()\n",
        "      image = sample['image']\n",
        "      w, h = image.size\n",
        "      \n",
        "      if random.random() < self.p:\n",
        "          for key in sample.keys():\n",
        "             if(key == 'image'):  \n",
        "                image = F.hflip(image)\n",
        "                items[key] = image\n",
        "             else:\n",
        "                all_dtls = sample[key]\n",
        "              \n",
        "                for i, dtls in enumerate(all_dtls):\n",
        "                   if dtls[2][0] > 0 and dtls[2][1] > 0:\n",
        "                      dtls[0][0] = w-1-dtls[0][0]-dtls[3][0]\n",
        "                      dtls[1][0] = w-1-dtls[1][0]+dtls[3][0]\n",
        "                      dtls[2][0] = w-1-dtls[2][0]\n",
        "\n",
        "                items[key] = all_dtls\n",
        "      else:\n",
        "         return sample\n",
        "      return items"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUGJG_DYYzEQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Normalizing the dataset\n",
        "class Normalize(object):\n",
        "    \"\"\"Normalize the image using mean and standard deviation provided.\n",
        "\n",
        "    Args:\n",
        "    mean(tensor): Mean of the guassian distribution to be used for normalization in \n",
        "    each dimension of the image.\n",
        "    std: Standard deviation of the guassian distribution to be used for normalization in \n",
        "    each dimension of the image.\n",
        "    \"\"\"\n",
        "    def __init__(self, mean, std):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        \"\"\" \n",
        "        Normalizes the image in each dimension as per the mean and std values.      \n",
        "        \"\"\"\n",
        "        items = dict()\n",
        "        for key in sample.keys():\n",
        "          if key == 'image':\n",
        "            image = sample[key]\n",
        "            items[key] = F.normalize(image, self.mean, self.std)\n",
        "          else:\n",
        "            items[key] = sample[key]\n",
        "        return items "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sl-BLYcuyA4J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Lambda(object):\n",
        "    \"\"\"Apply a user-defined lambda as a transform.\n",
        "\n",
        "    Args:\n",
        "    lambd (function): Lambda/function to be used for transform.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, lambd):\n",
        "        assert callable(lambd), repr(type(lambd).__name__) + \\\n",
        "            \" object is not callable\"\n",
        "        self.lambd = lambd\n",
        "\n",
        "    def __call__(self, img):\n",
        "        return self.lambd(img)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '()'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCAM2hLdm2ZS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ColorJitter(object):\n",
        "    \"\"\"Randomly change the brightness, contrast and saturation of an image.\n",
        "\n",
        "    Args:\n",
        "        brightness (float or tuple of float (min, max)): How much to jitter brightness.\n",
        "            brightness_factor is chosen uniformly from [max(0, 1 - brightness), 1 + brightness]\n",
        "            or the given [min, max]. Should be non negative numbers.\n",
        "        contrast (float or tuple of float (min, max)): How much to jitter contrast.\n",
        "            contrast_factor is chosen uniformly from [max(0, 1 - contrast), 1 + contrast]\n",
        "            or the given [min, max]. Should be non negative numbers.\n",
        "        saturation (float or tuple of float (min, max)): How much to jitter saturation.\n",
        "            saturation_factor is chosen uniformly from [max(0, 1 - saturation), 1 + saturation]\n",
        "            or the given [min, max]. Should be non negative numbers.\n",
        "        hue (float or tuple of float (min, max)): How much to jitter hue.\n",
        "            hue_factor is chosen uniformly from [-hue, hue] or the given [min, max].\n",
        "            Should have 0<= hue <= 0.5 or -0.5 <= min <= max <= 0.5.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, brightness=0, contrast=0, saturation=0, hue=0):\n",
        "        self.brightness = self._check_input(brightness, 'brightness')\n",
        "        self.contrast = self._check_input(contrast, 'contrast')\n",
        "        self.saturation = self._check_input(saturation, 'saturation')\n",
        "        self.hue = self._check_input(hue, 'hue', center=0, bound=(-0.5, 0.5),\n",
        "                                     clip_first_on_zero=False)\n",
        "\n",
        "    def _check_input(self, value, name, center=1, bound=(0, float('inf')), clip_first_on_zero=True):\n",
        "        if isinstance(value, numbers.Number):\n",
        "            if value < 0:\n",
        "                raise ValueError(\n",
        "                    \"If {} is a single number, it must be non negative.\".format(name))\n",
        "            value = [center - value, center + value]\n",
        "            if clip_first_on_zero:\n",
        "                value[0] = max(value[0], 0)\n",
        "        elif isinstance(value, (tuple, list)) and len(value) == 2:\n",
        "            if not bound[0] <= value[0] <= value[1] <= bound[1]:\n",
        "                raise ValueError(\n",
        "                    \"{} values should be between {}\".format(name, bound))\n",
        "        else:\n",
        "            raise TypeError(\n",
        "                \"{} should be a single number or a list/tuple with lenght 2.\".format(name))\n",
        "\n",
        "        # if value is 0 or (1., 1.) for brightness/contrast/saturation\n",
        "        # or (0., 0.) for hue, do nothing\n",
        "        if value[0] == value[1] == center:\n",
        "            value = None\n",
        "        return value\n",
        "\n",
        "    @staticmethod\n",
        "    def get_params(brightness, contrast, saturation, hue):\n",
        "        \"\"\"Get a randomized transform to be applied on image.\n",
        "\n",
        "        Arguments are same as that of __init__.\n",
        "\n",
        "        Returns:\n",
        "            Transform which randomly adjusts brightness, contrast and\n",
        "            saturation in a random order.\n",
        "        \"\"\"\n",
        "        tforms = []\n",
        "\n",
        "        if brightness is not None:\n",
        "            brightness_factor = random.uniform(brightness[0], brightness[1])\n",
        "            tforms.append(\n",
        "                Lambda(lambda img: F.adjust_brightness(img, brightness_factor)))\n",
        "\n",
        "        if contrast is not None:\n",
        "            contrast_factor = random.uniform(contrast[0], contrast[1])\n",
        "            tforms.append(\n",
        "                Lambda(lambda img: F.adjust_contrast(img, contrast_factor)))\n",
        "\n",
        "        if saturation is not None:\n",
        "            saturation_factor = random.uniform(saturation[0], saturation[1])\n",
        "            tforms.append(\n",
        "                Lambda(lambda img: F.adjust_saturation(img, saturation_factor)))\n",
        "\n",
        "        if hue is not None:\n",
        "            hue_factor = random.uniform(hue[0], hue[1])\n",
        "            tforms.append(Lambda(lambda img: F.adjust_hue(img, hue_factor)))\n",
        "\n",
        "        random.shuffle(tforms)\n",
        "        transform = transforms.Compose(tforms)\n",
        "\n",
        "        return transform\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            sample (List): List of Input image and bounding box\n",
        "\n",
        "        Returns:\n",
        "            List: Color jittered image and original bounding box.\n",
        "        \"\"\"\n",
        "        items = dict()\n",
        "        for key in sample.keys():\n",
        "          if key == 'image':\n",
        "            image = sample[key]\n",
        "            transform = self.get_params(self.brightness, self.contrast,\n",
        "                                    self.saturation, self.hue)\n",
        "            items[key] =  transform(image)\n",
        "          else:\n",
        "            items[key] = sample[key]\n",
        "        return items"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFfsEOJPZDrO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RobotDataset(Dataset):\n",
        "    \"\"\"Cutomized Dataset used to train the model.\n",
        "    Args:\n",
        "      root_dir: path where all the training files are saved.\n",
        "      transform: transformations to be applied to the dataset.\n",
        "      filenames: names of all the files in training dataset.\n",
        "    \"\"\"\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "            \n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.filenames = []\n",
        "        for file in os.listdir(root_dir):\n",
        "            if file.endswith(\".jpg\") or file.endswith('.jpeg'):\n",
        "               self.filenames.append(file)\n",
        "    def __len__(self):\n",
        "        \"\"\"Length of the dataset\"\"\"\n",
        "        return len(self.filenames)\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "        filepath =os.path.join(self.root_dir,self.filenames[ind])\n",
        "        image = Image.open(filepath)\n",
        "        if(filepath.endswith('.jpg')):\n",
        "          xml_data = ET.parse(filepath.replace('.jpg','.xml')).getroot()\n",
        "        elif(filepath.endswith('.jpeg')):\n",
        "          xml_data = ET.parse(filepath.replace('.jpeg','.xml')).getroot()        \n",
        "        all_dtls= []\n",
        "        \n",
        "        for group in xml_data.findall('object'):\n",
        "            bndvalues = group.find('bndbox')\n",
        "            all_dtls.append(find_bnddetails(bndvalues))       \n",
        "        sample = {'image': image, 'dtls': all_dtls}\n",
        "        if self.transform:\n",
        "            if type(self.transform) is not list:\n",
        "              self.transform = [self.transform]\n",
        "            for idx in range(len(self.transform)):\n",
        "                  sample = self.transform[idx](sample)\n",
        "        size = sample['image'].shape\n",
        "        heatmap_list = torch.zeros([4,int(size[1]/4),int(size[2]/4)])\n",
        "        i=0\n",
        "        for group in xml_data.findall('object'):\n",
        "            label = group.find('name').text\n",
        "            img_heatmap = HeatMap(sample['dtls'][i],(size[1],size[2]))\n",
        "            if(label == 'Head'):\n",
        "               heatmap_list[0] += img_heatmap\n",
        "            elif (label == 'Foot'):\n",
        "               heatmap_list[1] += img_heatmap \n",
        "            elif (label == 'Trunk'):\n",
        "               heatmap_list[2] += img_heatmap \n",
        "            elif (label == 'Hand'):\n",
        "               heatmap_list[3] += img_heatmap \n",
        "            i+=1  \n",
        "        dataset = {'image': sample['image'], 'heatmap': heatmap_list}\n",
        "        return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfDfIwbhZKY1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TestDataset(Dataset):\n",
        "\n",
        "    \"\"\"Customized dataset used for testing the model.\n",
        "\n",
        "    Args:\n",
        "        root_dir: path where all the training files are saved.\n",
        "        transform: transformations to be applied to the dataset.\n",
        "        filenames: names of all the files in training dataset.\n",
        "        \n",
        "    \"\"\"\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "            \n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.filenames = []\n",
        "        for file in os.listdir(root_dir):\n",
        "            if file.endswith(\".jpg\"):\n",
        "               self.filenames.append(file)\n",
        "    def __len__(self):\n",
        "        return len(self.filenames)\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "        filepath =os.path.join(self.root_dir,self.filenames[ind])\n",
        "        image = Image.open(filepath)\n",
        "        xml_data = ET.parse(filepath.replace('jpg','xml')).getroot()\n",
        "        head_dtls = np.zeros((3,4,2))\n",
        "        trunk_dtls = np.zeros((3,4,2))\n",
        "        hands_dtls = np.zeros((6,4,2))\n",
        "        foot_dtls = np.zeros((6,4,2))\n",
        "        head_idx = 0\n",
        "        trunk_idx = 0\n",
        "        hands_idx = 0\n",
        "        foot_idx = 0\n",
        "        for group in xml_data.findall('object'):\n",
        "            bndvalues = group.find('bndbox')\n",
        "            label = group.find('name').text\n",
        "            if(label == 'Head' and head_idx < 3):\n",
        "              head_dtls[head_idx] = find_bnddetails(bndvalues)\n",
        "              head_idx += 1\n",
        "            elif(label == 'Trunk' and trunk_idx < 3):\n",
        "              trunk_dtls[trunk_idx] = find_bnddetails(bndvalues)\n",
        "              trunk_idx += 1\n",
        "            elif(label == 'Foot' and foot_idx < 6):\n",
        "              foot_dtls[foot_idx] = find_bnddetails(bndvalues)\n",
        "              foot_idx += 1\n",
        "            elif(label == 'Hand' and hands_idx < 6):\n",
        "              hands_dtls[hands_idx] = find_bnddetails(bndvalues)\n",
        "              hands_idx += 1\n",
        "        sample = {'image': image, \n",
        "                  'head_dtls': head_dtls,\n",
        "                  'trunk_dtls':trunk_dtls,\n",
        "                  'hands_dtls':hands_dtls,\n",
        "                  'foot_dtls' : foot_dtls}\n",
        "        if self.transform:\n",
        "            if type(self.transform) is not list:\n",
        "              self.transform = [self.transform]\n",
        "            for idx in range(len(self.transform)):\n",
        "                  sample = self.transform[idx](sample)\n",
        "        \n",
        "        return sample"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RjJG1IngCfA",
        "colab_type": "text"
      },
      "source": [
        "**STEP 1: LOADING DATASET**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktZGGMM5Z-HC",
        "colab_type": "code",
        "outputId": "52beb378-26e9-401d-8378-46fbc0ffa0f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "#mounting google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClpGwoFVZexN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transforms1 = [Rescale((480,640)),RandomHorizontalFlip(), RandomVerticalFlip(),\n",
        "                ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
        "                ToTensor(), Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]\n",
        "transforms2 = [Rescale((480,640)),ToTensor(), Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]\n",
        "train_dataset = RobotDataset(root_dir = '/content/drive/My Drive/Final_dataset/Train',transform=transforms1)\n",
        "test_dataset = TestDataset(root_dir = '/content/drive/My Drive/Final_dataset/Test1',transform=transforms2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BiROy0_WgIqa",
        "colab_type": "text"
      },
      "source": [
        "**STEP 2: MAKING DATASET ITERABLE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CiS088MY1-Fu",
        "colab_type": "code",
        "outputId": "7868cc66-e38d-4185-915a-429c195affc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Length of training dataset\n",
        "len(train_dataset)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2128"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ileyr4I5hY7G",
        "colab_type": "code",
        "outputId": "5fabd736-2c55-49dc-c636-027952557bee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Length of testing dataset\n",
        "len(test_dataset)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "818"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hucTmeVjZiC6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#batch size\n",
        "batch_size = 5\n",
        "\n",
        "#set device to GPU\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "testloader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VVfo_kOgOFp",
        "colab_type": "text"
      },
      "source": [
        "**STEP 3: CREATE MODEL CLASS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNPfsXCKZ12x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class myModel(nn.Module):\n",
        "      \"\"\"\n",
        "        Model for body parts detection\n",
        "      \"\"\"\n",
        "      def __init__(self):\n",
        "        super(myModel,self).__init__()\n",
        "        model_resnet = models.resnet18(pretrained=True)\n",
        "        self.encoder_block1 = nn.Sequential(*list(model_resnet.children())[0:5])\n",
        "        self.encoder_block2 = nn.Sequential(*list(model_resnet.children())[5:6])\n",
        "        self.encoder_block3 = nn.Sequential(*list(model_resnet.children())[6:7])\n",
        "        self.encoder_block4 = nn.Sequential(*list(model_resnet.children())[7:-2])\n",
        "         \n",
        "        self.decoder_block1 = nn.Sequential(\n",
        "                        nn.ReLU(),\n",
        "                        nn.ConvTranspose2d(512,128,3,2,1,output_padding = 1))\n",
        "                             \n",
        "        self.decoder_block2 = nn.Sequential(\n",
        "                        nn.ReLU(),\n",
        "                        nn.BatchNorm2d(256),\n",
        "                        nn.ConvTranspose2d(256,128,3,2,1,output_padding = 1))\n",
        "                        \n",
        "        self.decoder_block3 = nn.Sequential(\n",
        "                        nn.ReLU(),\n",
        "                        nn.BatchNorm2d(256),\n",
        "                        nn.ConvTranspose2d(256,128,3,2,1,output_padding = 1))\n",
        "                        \n",
        "        self.decoder_block4 = nn.Sequential(\n",
        "                        nn.ReLU(),\n",
        "                        nn.BatchNorm2d(256),\n",
        "                        nn.ConvTranspose2d(256,4,3,1,1))\n",
        "        self.conv1x1_1 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=1)\n",
        "        self.conv1x1_2 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=1)\n",
        "        self.conv1x1_3 = nn.Conv2d(in_channels=256, out_channels=128, kernel_size=1)\n",
        "         \n",
        "      \n",
        "\n",
        "      def forward(self,x):\n",
        "        out = self.encoder_block1(x) #64x120x160\n",
        "        residual1 = self.conv1x1_1(out) #128x120x160\n",
        "         \n",
        "        out = self.encoder_block2(out)  #128x60x80\n",
        "         \n",
        "        residual2 = self.conv1x1_2(out)#128x60x80\n",
        "         \n",
        "        out = self.encoder_block3(out) #256x30x40\n",
        "         \n",
        "        residual3 = self.conv1x1_3(out) #128x30x40\n",
        "        out = self.encoder_block4(out) #512x15x20\n",
        "        \n",
        "        out = self.decoder_block1(out) #128x30x40\n",
        "        \n",
        "        \n",
        "        decoder_block2_input = torch.cat((out,residual3), 1) \n",
        "        \n",
        "        out = self.decoder_block2(decoder_block2_input)\n",
        "        \n",
        "        decoder_block3_input = torch.cat((out,residual2),1) \n",
        "        \n",
        "        out = self.decoder_block3(decoder_block3_input) \n",
        "        decoder_block4_input = torch.cat((out,residual1),1) \n",
        "        out = self.decoder_block4(decoder_block4_input) \n",
        "        return out        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bB9LrFPgUCh",
        "colab_type": "text"
      },
      "source": [
        "**STEP 4: INSTANTIATE MODEL CLASS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-mox-TWdc1I",
        "colab_type": "code",
        "outputId": "3f059a71-2bfe-4ea1-b4f5-0cd5fb01d5ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#to instantiate and use the model\n",
        "model = myModel()\n",
        "model.to(device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/checkpoints/resnet18-5c106cde.pth\n",
            "100%|██████████| 44.7M/44.7M [00:01<00:00, 26.2MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "myModel(\n",
              "  (encoder_block1): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "    (4): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (encoder_block2): Sequential(\n",
              "    (0): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (encoder_block3): Sequential(\n",
              "    (0): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (encoder_block4): Sequential(\n",
              "    (0): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (decoder_block1): Sequential(\n",
              "    (0): ReLU()\n",
              "    (1): ConvTranspose2d(512, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
              "  )\n",
              "  (decoder_block2): Sequential(\n",
              "    (0): ReLU()\n",
              "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
              "  )\n",
              "  (decoder_block3): Sequential(\n",
              "    (0): ReLU()\n",
              "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
              "  )\n",
              "  (decoder_block4): Sequential(\n",
              "    (0): ReLU()\n",
              "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ConvTranspose2d(256, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  )\n",
              "  (conv1x1_1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "  (conv1x1_2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "  (conv1x1_3): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hssx2gWTgdeP",
        "colab_type": "text"
      },
      "source": [
        "**STEP 5: INSTANTIATE LOSS CLASS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0edqFdg-Zh-U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Visualization of some of the training dataset images and corresponding heat-maps\n",
        "k=1\n",
        "for j,train_data in enumerate(train_loader):\n",
        "      k +=1\n",
        "      images = train_data['image'].to(device)\n",
        "      heatmaps = train_data['heatmap'].to(device)\n",
        "      plt.subplot(2,3,1)\n",
        "      plt.title('Train image' + str(k))\n",
        "      plt.imshow(images[0][0].cpu())\n",
        "      plt.subplot(2,3,2)\n",
        "      plt.title('Head')\n",
        "      plt.imshow(heatmaps[0][0].cpu().detach().numpy(), cmap= 'gray')\n",
        "      plt.subplot(2,3,3)\n",
        "      plt.title('Foot')\n",
        "      plt.imshow(heatmaps[0][1].cpu().detach().numpy(), cmap= 'gray')\n",
        "      plt.subplot(2,3,5)\n",
        "      plt.title('Trunk')\n",
        "      plt.imshow(heatmaps[0][2].cpu().detach().numpy(), cmap= 'gray')\n",
        "      plt.subplot(2,3,6)\n",
        "      plt.title('Hands')\n",
        "      plt.imshow(heatmaps[0][3].cpu().detach().numpy(), cmap= 'gray')\n",
        "      plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlYG-lgMd6pl",
        "colab_type": "code",
        "outputId": "702bb939-b95a-46c7-d626-7bb1265f69cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Loss Function\n",
        "criterion = nn.MSELoss()\n",
        "criterion.to(device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MSELoss()"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdBxJtVdghY9",
        "colab_type": "text"
      },
      "source": [
        "**STEP 6: INSTANTIATE OPTIMIZER CLASS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0p8JQUOqd2j2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Optimizer intialization\n",
        "optimizer = optim.Adam([\n",
        "                        {\"params\":model.encoder_block1.parameters(), \"lr\": 0.000001},\n",
        "                        {\"params\":model.encoder_block2.parameters(), \"lr\": 0.000001},\n",
        "                        {\"params\":model.encoder_block3.parameters(), \"lr\": 0.000001},\n",
        "                        {\"params\":model.encoder_block4.parameters(), \"lr\": 0.000001},\n",
        "                        {\"params\":model.decoder_block1.parameters()},\n",
        "                        {\"params\":model.decoder_block2.parameters()},\n",
        "                        {\"params\":model.decoder_block3.parameters()},\n",
        "                        {\"params\":model.decoder_block4.parameters()},\n",
        "                        {\"params\":model.conv1x1_1.parameters()},\n",
        "                        {\"params\":model.conv1x1_2.parameters()},\n",
        "                        {\"params\":model.conv1x1_3.parameters()}], lr=0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPPRkf4hd0BY",
        "colab_type": "text"
      },
      "source": [
        "**STEP 7: TRAIN THE MODEL**\n",
        "1. Functions used for training and testing the model.\n",
        "2. Training the model for 100 epochs with normal and downsampled datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0ufxJp2Wapl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def train_model(model,train_loader, criterion, optimizer):\n",
        "      \"\"\"\n",
        "        Function used to train the model\n",
        "        Args:\n",
        "          model: Model which is to be trained.\n",
        "          Train_loader: Batched dataset used to train the model.\n",
        "          criterion: Criterion used to train the model.\n",
        "          optimizer: optimizer used for the model.\n",
        "      \"\"\"      \n",
        "      \n",
        "      model.train()\n",
        "      train_loss = 0.0\n",
        "      for i_batch, sample_batched in enumerate(train_loader):\n",
        "          \n",
        "          image = sample_batched['image'].to(device)\n",
        "          heatmap = sample_batched['heatmap'].to(device) \n",
        "          \n",
        "          output = model(image)\n",
        "          optimizer.zero_grad()\n",
        "          loss = criterion(output, heatmap)\n",
        "          train_loss += loss\n",
        "          loss.backward()  \n",
        "          optimizer.step()\n",
        "          \n",
        "      return train_loss.item(), model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulXBv7X8knS_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Training the network \n",
        "#Only once\n",
        "total_loss = 0.0\n",
        "loss_details = [None]*100\n",
        "for epoch in range(100):\n",
        "    \n",
        "  \n",
        "    print(\"epoch\",epoch)\n",
        "    train_loss_, model = train_model(model,train_loader, criterion, optimizer)\n",
        "    loss_details[epoch] = train_loss_\n",
        "    print(\"train loss\",train_loss_)\n",
        "    total_loss += train_loss_\n",
        "    print(\"total loss\", total_loss)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELLN6ha3q1Kb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path1 = '/content/drive/My Drive/Final_dataset/Final_model.pth'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGTTJYSdrM6S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Saving details of trained model\n",
        "#Only once\n",
        "torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'train_loss1': loss_details,\n",
        "            }, path1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVGWZHn9rl8w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = myModel()\n",
        "optimizer = optimizer\n",
        "checkpoint = torch.load(path1)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "epoch = checkpoint['epoch']\n",
        "train_loss_1 = checkpoint['train_loss1']\n",
        "model.to(device)\n",
        "#model.eval()\n",
        "# - or -\n",
        "model.train()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPkh-PtLxmWd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Only once\n",
        "total_loss1 = 0.0\n",
        "\n",
        "\n",
        "loss_details1 = [None]*150\n",
        "for epoch in range(100,150):\n",
        "    \n",
        "  \n",
        "    print(\"epoch\",epoch)\n",
        "    train_loss_, model = train_model(model,train_loader, criterion, optimizer,\n",
        "                                     epoch)\n",
        "    loss_details[epoch] = train_loss_\n",
        "    print(\"train loss\",train_loss_)\n",
        "    total_loss1 += train_loss_\n",
        "    print(\"total loss\", total_loss)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbT2uaUGIUir",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path2 = '/content/drive/My Drive/Final_dataset/Final_model1.pth'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNz72WnyIicM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#To save the model\n",
        "#Only once\n",
        "torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'train_loss1': train_loss_1,\n",
        "    'train_loss2' : loss_details,\n",
        "            }, path2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wi1zx6_gGySU",
        "colab_type": "text"
      },
      "source": [
        "**Evaluating the network**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fezPKorQGx3s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = myModel()\n",
        "optimizer = optimizer\n",
        "\n",
        "checkpoint = torch.load(path2)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "epoch = checkpoint['epoch']\n",
        "train_loss_1 = checkpoint['train_loss1']\n",
        "\n",
        "model.to(device)\n",
        "model.eval()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0r-JTuMeG4t",
        "colab_type": "text"
      },
      "source": [
        "**Functions used for testing the network**\n",
        "1.Calculate the center of the contour\n",
        "2.Post process function to calculate the metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fed9lZo4eHCF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def center_of_shape(image):\n",
        "  \"\"\"\n",
        "  To find centers of the contours in the input image.\n",
        "  \n",
        "  Args:\n",
        "  image: Image of which we want to find the contours.\n",
        "  \n",
        "  Returns: Array of centers of all the contours in the input image.\n",
        "  \"\"\"\n",
        "  out_centers = []\n",
        "  kernel = np.ones((3, 3), np.uint8)\n",
        "  blurred = cv2.GaussianBlur(image, (5, 5), 0)\n",
        "  thresh = cv2.threshold(blurred,0.7, 255, cv2.THRESH_BINARY)[1]\n",
        "  cnts = cv2.findContours(thresh.copy().astype(np.uint8), cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
        "  cnts = imutils.grab_contours(cnts)\n",
        "  for c in cnts:\n",
        "\t# compute the center of the contour\n",
        "    M = cv2.moments(c)\n",
        "    if(M[\"m00\"]>0):\n",
        "      cX = int(M[\"m10\"] / M[\"m00\"])\n",
        "      cY = int(M[\"m01\"] / M[\"m00\"])\n",
        "      out_centers.append((cX,cY))\n",
        "\n",
        "  return torch.FloatTensor(out_centers)\n",
        "\t# draw the contour and center of the shape on the image\n",
        "\t"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5X7unTn5mzv_",
        "colab_type": "text"
      },
      "source": [
        "**Thresholding the distance with less than 4 pixels**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dv9kbICVfK-N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def post_process(tensor,out,allowed_distance):\n",
        "  \"\"\"\n",
        "  Determinining if the output from the model are contributing to true positive,\n",
        "  false negative or false positive based on the distance between the output and \n",
        "  actual center.\n",
        "  \n",
        "  Args: \n",
        "  Tensor: Boundary box details from the test dataset.\n",
        "  out: Centers of output details from the contour from output of the model.\n",
        "  allowed_distance: acceptable distance between boundary box center and output center.\n",
        "  returns:\n",
        "  Number of True Psoitive, False Negative and False postives in given combination.\n",
        "  \n",
        "  \"\"\"\n",
        "  l_b = 0\n",
        "  l_o = len(out)\n",
        "  \n",
        "  TruePositive = 0\n",
        "  FalseNegative = 0\n",
        "  FalsePositive = 0\n",
        "  correct = 0\n",
        "  for i in range(tensor.size(0)):\n",
        "    if(tensor[i][2][0]>0 or tensor[i][2][1]>0):\n",
        "      l_b +=1\n",
        "  \n",
        "  flag = np.zeros(l_b)\n",
        "  for k in range(l_o):\n",
        "    found = -1\n",
        "    for l in range(l_b):\n",
        "      Boundary_center = tensor[l][2]/4.0\n",
        "      object_detected = False\n",
        "      out_center = out[k]\n",
        "      if out_center[0]>0 or out_center[1]>0:\n",
        "        object_detected = True\n",
        "       \n",
        "      if object_detected:\n",
        "        dist = distance.euclidean(Boundary_center.cpu(),out_center.cpu()) \n",
        "        if dist <= allowed_distance:  \n",
        "          TruePositive +=1\n",
        "          found = l\n",
        "          break\n",
        "    if found == -1:\n",
        "      FalsePositive += 1\n",
        "    else:\n",
        "      flag[found] = 1\n",
        "  FalseNegative = np.count_nonzero(flag == 0)\n",
        "  return TruePositive,FalseNegative,FalsePositive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJC_9kNlomzM",
        "colab_type": "text"
      },
      "source": [
        "**Thresholding distance less than 4 pixel**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6y3nFDeM4qpq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Testing the model\n",
        "#loaded the model to use it. Therefore model.eval() is used\n",
        "#To used the trained model. model_final() should be used\n",
        "#distance = 4\n",
        "model.eval()\n",
        "correct = 0\n",
        "test_loss = 0\n",
        "total = 0\n",
        "Recall = torch.zeros(4)\n",
        "TruePositive = torch.zeros(4)\n",
        "FalsePositive = torch.zeros(4)\n",
        "FalseNegative = torch.zeros(4)\n",
        "FDR = torch.zeros(4)\n",
        "acc = torch.zeros(len(testloader))\n",
        "\n",
        "for j,test_data in enumerate(testloader):\n",
        "      \n",
        "    images = test_data['image'].to(device)\n",
        "    head_dtls = test_data['head_dtls'].to(device)\n",
        "    trunk_dtls = test_data['trunk_dtls'].to(device)\n",
        "    hands_dtls = test_data['hands_dtls'].to(device)\n",
        "    foot_dtls = test_data['foot_dtls'].to(device)\n",
        "    images.requires_grad_(False)\n",
        "    head_dtls.requires_grad_(False)\n",
        "    trunk_dtls.requires_grad_(False)\n",
        "    hands_dtls.requires_grad_(False)\n",
        "    foot_dtls.requires_grad_(False)\n",
        "    outputs = model(images)\n",
        "    batch_size = outputs.shape[0]\n",
        "    \n",
        "    for i in range(batch_size):\n",
        "      out_center1 = center_of_shape(outputs[i][0].cpu().detach().numpy())\n",
        "      tp,fn,fp = post_process(head_dtls[i],out_center1,4)\n",
        "      TruePositive[0] += tp\n",
        "      FalseNegative[0] += fn\n",
        "      FalsePositive[0]  += fp\n",
        "      \n",
        "      out_center2 = center_of_shape(outputs[i][1].cpu().detach().numpy())\n",
        "      tp,fn,fp = post_process(foot_dtls[i],out_center2,4)\n",
        "      \n",
        "      TruePositive[1] += tp\n",
        "      FalseNegative[1] += fn\n",
        "      FalsePositive[1]  += fp\n",
        "\n",
        "      out_center3 = center_of_shape(outputs[i][2].cpu().detach().numpy())\n",
        "      tp,fn,fp = post_process(trunk_dtls[i],out_center3,4)\n",
        "      \n",
        "      TruePositive[2] += tp\n",
        "      FalseNegative[2] += fn\n",
        "      FalsePositive[2]  += fp\n",
        "      out_center4 = center_of_shape(outputs[i][3].cpu().detach().numpy())\n",
        "      tp,fn,fp = post_process(hands_dtls[i],out_center4,4)\n",
        "      \n",
        "      TruePositive[3] += tp\n",
        "      FalseNegative[3] += fn\n",
        "      FalsePositive[3]  += fp\n",
        "    acc[j] = torch.mean(TruePositive) / (torch.mean(TruePositive) + torch.mean(FalseNegative) + torch.mean(FalsePositive))\n",
        "\n",
        "for i in range(4):\n",
        "    Recall[i] = (TruePositive[i])/(TruePositive[i]+FalseNegative[i])\n",
        "    FDR[i] = (FalsePositive[i])/(FalsePositive[i]+TruePositive[i])\n",
        "\n",
        "\n",
        "print(\"Recall of Head \"+str(round(float(Recall[0]),2)) )\n",
        "print(\"Recall of Foot \"+str(round(float(Recall[1] ),2)))\n",
        "print(\"Recall of Trunk \"+str(round(float(Recall[2]),2)) )\n",
        "print(\"Recall of Hand \"+str(round(float(Recall[3]),2)) )\n",
        "print(\"FDR of Head \"+str(round(float(FDR[0] ),2)))\n",
        "print(\"FDR of Foot \"+str(round(float(FDR[1]),2)) )\n",
        "print(\"FDR of Trunk \"+str(round(float(FDR[2]),2)) )\n",
        "print(\"FDR of Hand \"+str(round(float(FDR[3] ),2)))\n",
        "\n",
        "\n",
        "Total_recall = torch.mean(Recall)\n",
        "Total_FDR = torch.mean(FDR)\n",
        "print(\"Total recall \"  +str(round(float(Total_recall),2)))\n",
        "print(\"Total FDR \"  +str(round(float(Total_FDR),2)))\n",
        "print(\"Accuracy \" +str(round(float(acc.mean()),2)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKpGm-BPWQJy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "acc.mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uODAGhUgnn4z",
        "colab_type": "text"
      },
      "source": [
        "**Thresholding the distance with less than 5 pixels**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ch6Odb4oaG0H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ploting some of the test images along with output from different channels\n",
        "\n",
        "k=1\n",
        "for j,test_data in enumerate(testloader):\n",
        "      \n",
        "    images = test_data['image'].to(device)\n",
        "    head_dtls = test_data['head_dtls'].to(device)\n",
        "    trunk_dtls = test_data['trunk_dtls'].to(device)\n",
        "    hands_dtls = test_data['hands_dtls'].to(device)\n",
        "    foot_dtls = test_data['foot_dtls'].to(device)\n",
        "    images.requires_grad_(False)\n",
        "    head_dtls.requires_grad_(False)\n",
        "    trunk_dtls.requires_grad_(False)\n",
        "    hands_dtls.requires_grad_(False)\n",
        "    foot_dtls.requires_grad_(False)\n",
        "      #Forward pass only to get logits/output\n",
        "    batch_size = outputs.shape[0]\n",
        "    \n",
        "    for i in range(batch_size):\n",
        "      k += 1\n",
        "      plt.subplot(2,3,1)\n",
        "      plt.title('Test image' + str(k))\n",
        "      plt.imshow(images[i][0].cpu())\n",
        "      outputs = model(images)\n",
        "      plt.subplot(2,3,2)\n",
        "      plt.title('Head')\n",
        "      plt.imshow(outputs[i][0].cpu().detach().numpy(), cmap= 'gray')\n",
        "      plt.subplot(2,3,3)\n",
        "      plt.title('Foot')\n",
        "      plt.imshow(outputs[i][1].cpu().detach().numpy(), cmap= 'gray')\n",
        "      plt.subplot(2,3,5)\n",
        "      plt.title('Trunk')\n",
        "      plt.imshow(outputs[i][2].cpu().detach().numpy(), cmap= 'gray')\n",
        "      plt.subplot(2,3,6)\n",
        "      plt.title('Hands')\n",
        "      plt.imshow(outputs[i][3].cpu().detach().numpy(), cmap= 'gray')\n",
        "      plt.show()\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qgCTjxeD05C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Testing the model\n",
        "#loaded the model to use it. Therefore model.eval() is used\n",
        "#To used the trained model. model_final() should be used\n",
        "#distance = 5\n",
        "model.eval()\n",
        "correct = 0\n",
        "test_loss = 0\n",
        "total = 0\n",
        "Recall = torch.zeros(4)\n",
        "TruePositive = torch.zeros(4)\n",
        "FalsePositive = torch.zeros(4)\n",
        "FalseNegative = torch.zeros(4)\n",
        "FDR = torch.zeros(4)\n",
        "acc_1 = torch.zeros(len(testloader))\n",
        "\n",
        "for j,test_data in enumerate(testloader):\n",
        "      \n",
        "    images = test_data['image'].to(device)\n",
        "    head_dtls = test_data['head_dtls'].to(device)\n",
        "    trunk_dtls = test_data['trunk_dtls'].to(device)\n",
        "    hands_dtls = test_data['hands_dtls'].to(device)\n",
        "    foot_dtls = test_data['foot_dtls'].to(device)\n",
        "    images.requires_grad_(False)\n",
        "    head_dtls.requires_grad_(False)\n",
        "    trunk_dtls.requires_grad_(False)\n",
        "    hands_dtls.requires_grad_(False)\n",
        "    foot_dtls.requires_grad_(False)\n",
        "    outputs = model(images)\n",
        "    batch_size = outputs.shape[0]\n",
        "    \n",
        "    for i in range(batch_size):\n",
        "      out_center1 = center_of_shape(outputs[i][0].cpu().detach().numpy())\n",
        "      tp,fn,fp = post_process(head_dtls[i],out_center1,5)\n",
        "      TruePositive[0] += tp\n",
        "      FalseNegative[0] += fn\n",
        "      FalsePositive[0]  += fp\n",
        "      \n",
        "      out_center2 = center_of_shape(outputs[i][1].cpu().detach().numpy())\n",
        "      tp,fn,fp = post_process(foot_dtls[i],out_center2,5)\n",
        "      \n",
        "      TruePositive[1] += tp\n",
        "      FalseNegative[1] += fn\n",
        "      FalsePositive[1]  += fp\n",
        "\n",
        "      out_center3 = center_of_shape(outputs[i][2].cpu().detach().numpy())\n",
        "      tp,fn,fp = post_process(trunk_dtls[i],out_center3,5)\n",
        "      \n",
        "      TruePositive[2] += tp\n",
        "      FalseNegative[2] += fn\n",
        "      FalsePositive[2]  += fp\n",
        "      \n",
        "      out_center4 = center_of_shape(outputs[i][3].cpu().detach().numpy())\n",
        "      tp,fn,fp = post_process(hands_dtls[i],out_center4,5)\n",
        "      \n",
        "      TruePositive[3] += tp\n",
        "      FalseNegative[3] += fn\n",
        "      FalsePositive[3]  += fp\n",
        "    acc_1[j] = torch.mean(TruePositive) / (torch.mean(TruePositive) + torch.mean(FalseNegative) + torch.mean(FalsePositive))\n",
        "\n",
        "for i in range(4):\n",
        "    Recall[i] = (TruePositive[i])/(TruePositive[i]+FalseNegative[i])\n",
        "    FDR[i] = (FalsePositive[i])/(FalsePositive[i]+TruePositive[i])\n",
        "\n",
        "\n",
        "print(\"Recall of Head \"+str(round(float(Recall[0]),2)) )\n",
        "print(\"Recall of Foot \"+str(round(float(Recall[1] ),2)))\n",
        "print(\"Recall of Trunk \"+str(round(float(Recall[2]),2)) )\n",
        "print(\"Recall of Hand \"+str(round(float(Recall[3]),2)) )\n",
        "print(\"FDR of Head \"+str(round(float(FDR[0] ),2)))\n",
        "print(\"FDR of Foot \"+str(round(float(FDR[1]),2)) )\n",
        "print(\"FDR of Trunk \"+str(round(float(FDR[2]),2)) )\n",
        "print(\"FDR of Hand \"+str(round(float(FDR[3] ),2)))\n",
        "\n",
        "\n",
        "Total_recall = torch.mean(Recall)\n",
        "Total_FDR = torch.mean(FDR)\n",
        "print(\"Total recall \"  +str(round(float(Total_recall),2)))\n",
        "print(\"Total FDR \"  +str(round(float(Total_FDR),2)))\n",
        "print(\"Accuracy \" +str(round(float(acc_1.mean()),2)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZHJUBi5DZoD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Average accuracy\n",
        "acc_1.mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8tqq35FoUrK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Testing the model\n",
        "#loaded the model to use it. Therefore model.eval() is used\n",
        "#To used the trained model. model_final() should be used\n",
        "#distance = 6\n",
        "model.eval()\n",
        "correct = 0\n",
        "test_loss = 0\n",
        "total = 0\n",
        "Recall = torch.zeros(4)\n",
        "TruePositive = torch.zeros(4)\n",
        "FalsePositive = torch.zeros(4)\n",
        "FalseNegative = torch.zeros(4)\n",
        "FDR = torch.zeros(4)\n",
        "acc_2 = torch.zeros(len(testloader))\n",
        "\n",
        "for j,test_data in enumerate(testloader):\n",
        "      \n",
        "    images = test_data['image'].to(device)\n",
        "    head_dtls = test_data['head_dtls'].to(device)\n",
        "    trunk_dtls = test_data['trunk_dtls'].to(device)\n",
        "    hands_dtls = test_data['hands_dtls'].to(device)\n",
        "    foot_dtls = test_data['foot_dtls'].to(device)\n",
        "    images.requires_grad_(False)\n",
        "    head_dtls.requires_grad_(False)\n",
        "    trunk_dtls.requires_grad_(False)\n",
        "    hands_dtls.requires_grad_(False)\n",
        "    foot_dtls.requires_grad_(False)\n",
        "    outputs = model(images)\n",
        "    batch_size = outputs.shape[0]\n",
        "    \n",
        "    for i in range(batch_size):\n",
        "      out_center1 = center_of_shape(outputs[i][0].cpu().detach().numpy())\n",
        "      tp,fn,fp = post_process(head_dtls[i],out_center1,6)\n",
        "      TruePositive[0] += tp\n",
        "      FalseNegative[0] += fn\n",
        "      FalsePositive[0]  += fp\n",
        "      \n",
        "      out_center2 = center_of_shape(outputs[i][1].cpu().detach().numpy())\n",
        "      tp,fn,fp = post_process(foot_dtls[i],out_center2,6)\n",
        "      \n",
        "      TruePositive[1] += tp\n",
        "      FalseNegative[1] += fn\n",
        "      FalsePositive[1]  += fp\n",
        "\n",
        "      out_center3 = center_of_shape(outputs[i][2].cpu().detach().numpy())\n",
        "      tp,fn,fp = post_process(trunk_dtls[i],out_center3,6)\n",
        "      \n",
        "      TruePositive[2] += tp\n",
        "      FalseNegative[2] += fn\n",
        "      FalsePositive[2]  += fp\n",
        "      out_center4 = center_of_shape(outputs[i][3].cpu().detach().numpy())\n",
        "      tp,fn,fp = post_process(hands_dtls[i],out_center4,6)\n",
        "      \n",
        "      TruePositive[3] += tp\n",
        "      FalseNegative[3] += fn\n",
        "      FalsePositive[3]  += fp\n",
        "    acc_2[j] = torch.mean(TruePositive) / (torch.mean(TruePositive) + torch.mean(FalseNegative) + torch.mean(FalsePositive))\n",
        "\n",
        "for i in range(4):\n",
        "    Recall[i] = (TruePositive[i])/(TruePositive[i]+FalseNegative[i])\n",
        "    FDR[i] = (FalsePositive[i])/(FalsePositive[i]+TruePositive[i])\n",
        "\n",
        "\n",
        "print(\"Recall of Head \"+str(round(float(Recall[0]),2)) )\n",
        "print(\"Recall of Foot \"+str(round(float(Recall[1] ),2)))\n",
        "print(\"Recall of Trunk \"+str(round(float(Recall[2]),2)) )\n",
        "print(\"Recall of Hand \"+str(round(float(Recall[3]),2)) )\n",
        "print(\"FDR of Head \"+str(round(float(FDR[0] ),2)))\n",
        "print(\"FDR of Foot \"+str(round(float(FDR[1]),2)) )\n",
        "print(\"FDR of Trunk \"+str(round(float(FDR[2]),2)) )\n",
        "print(\"FDR of Hand \"+str(round(float(FDR[3] ),2)))\n",
        "\n",
        "\n",
        "Total_recall = torch.mean(Recall)\n",
        "Total_FDR = torch.mean(FDR)\n",
        "print(\"Total recall \"  +str(round(float(Total_recall),2)))\n",
        "print(\"Total FDR \"  +str(round(float(Total_FDR),2)))\n",
        "print(\"Accuracy \" +str(round(float(acc_2.mean()),2)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8xehuXmobcR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "acc_2.mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyfqjsiYaC4z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Visulaizing output from all the channels for all the images in test dataset\n",
        "model.eval()\n",
        "correct = 0\n",
        "test_loss = 0\n",
        "total = 0\n",
        "Recall = torch.zeros(4)\n",
        "TruePositive = torch.zeros(4)\n",
        "FalsePositive = torch.zeros(4)\n",
        "FalseNegative = torch.zeros(4)\n",
        "FDR = torch.zeros(4)\n",
        "acc = torch.zeros(len(testloader))\n",
        "k=1\n",
        "for j,test_data in enumerate(testloader):\n",
        "      \n",
        "    images = test_data['image'].to(device)\n",
        "    head_dtls = test_data['head_dtls'].to(device)\n",
        "    trunk_dtls = test_data['trunk_dtls'].to(device)\n",
        "    hands_dtls = test_data['hands_dtls'].to(device)\n",
        "    foot_dtls = test_data['foot_dtls'].to(device)\n",
        "    images.requires_grad_(False)\n",
        "    head_dtls.requires_grad_(False)\n",
        "    trunk_dtls.requires_grad_(False)\n",
        "    hands_dtls.requires_grad_(False)\n",
        "    foot_dtls.requires_grad_(False)\n",
        "      #Forward pass only to get logits/output\n",
        "    batch_size = outputs.shape[0]\n",
        "    \n",
        "    for i in range(batch_size):\n",
        "      k += 1\n",
        "      plt.subplot(2,3,1)\n",
        "      plt.title('Test image' + str(k))\n",
        "      plt.imshow(images[i][0].cpu())\n",
        "      outputs = model(images)\n",
        "      plt.subplot(2,3,2)\n",
        "      plt.title('Head')\n",
        "      plt.imshow(outputs[i][0].cpu().detach().numpy(), cmap= 'gray')\n",
        "      plt.subplot(2,3,3)\n",
        "      plt.title('Foot')\n",
        "      plt.imshow(outputs[i][1].cpu().detach().numpy(), cmap= 'gray')\n",
        "      plt.subplot(2,3,5)\n",
        "      plt.title('Trunk')\n",
        "      plt.imshow(outputs[i][2].cpu().detach().numpy(), cmap= 'gray')\n",
        "      plt.subplot(2,3,6)\n",
        "      plt.title('Hands')\n",
        "      plt.imshow(outputs[i][3].cpu().detach().numpy(), cmap= 'gray')\n",
        "      plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eu3jJoaoYbd",
        "colab_type": "text"
      },
      "source": [
        "**Learning Curve**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvHdQHjonOZu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = range(0,100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crd9bFyqkGl9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loss = train_loss_1[:100]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9lL1Ie0k2QZ",
        "colab_type": "code",
        "outputId": "7eca8681-2ceb-458d-8d6c-845070173233",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "plt.plot(epochs, train_loss)\n",
        "plt.ylabel('Train loss')\n",
        "plt.xlabel('No of Epochs')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAHeJJREFUeJzt3XmQHHd99/H3p3t2V7d1eNHjAyMT\n/EA5FD5YjCkID9hAjKGwi4fDFEVU4OdRpXJwJuAcVZDnefI8EAhXiqJK8YHC4SOOjQ0hEEeYEAqw\nvbKEMRZBtsGHkK01PmTJ0u7OzPf5o3tW49XM7Fi7vSt1f17lrZnu6en+tdq1n/31t/vXigjMzKy6\nkoVugJmZLSwHgZlZxTkIzMwqzkFgZlZxDgIzs4pzEJiZVZyDwMys4hwEZmYV5yAwM6u42kI3oB/H\nHntsrFu3bqGbYWZ2VNmyZcsjETE803JHRRCsW7eO0dHRhW6GmdlRRdJ9/SznU0NmZhXnIDAzqzgH\ngZlZxRUaBJI+IOlnku6UdKWkRZJOlnSLpLslXS1psMg2mJlZb4UFgaQTgPcCIxHxQiAFLgI+AXwm\nIp4HPAZcXFQbzMxsZkWfGqoBiyXVgCXALuAc4Nr8803AhQW3wczMeigsCCJiJ/Ap4H6yAHgC2AI8\nHhH1fLEHgRM6fV/SBkmjkkbHxsaKaqaZWeUVeWpoFXABcDJwPLAUOK/f70fExogYiYiR4eEZ74fo\n6PqtD/KVH/d1Ga2ZWWUVeWroNcAvI2IsIiaB64CXAyvzU0UAJwI7i2rAN36yi6tve6Co1ZuZlUKR\nQXA/cLakJZIEnAvcBdwMvCVfZj1wQ1ENSBNRb0ZRqzczK4UiawS3kBWFbwd+mm9rI/AR4IOS7gbW\nAJcV1YZaIhrNZlGrNzMrhULHGoqIjwIfnTb7XuCsIrfbkiai3nCPwMysl1LfWVzzqSEzsxmVOwjS\nhIaDwMysp3IHQSLqrhGYmfVU6iBwjcDMbGalDgLXCMzMZlbuIHCNwMxsRuUOAtcIzMxmVOogSBO5\nR2BmNoNSB0EtEZONIMJhYGbWTbmDIM12z50CM7PuSh0EaSIA1wnMzHoodRDU8iBwncDMrLtSB0Gr\nRzDpm8rMzLoqdRC4R2BmNrNyB0FeLHaNwMysu3IHgXsEZmYzKvLh9c+XtK3tZ4+k90taLekmSTvy\n11VFtWHqqiHXCMzMuiryUZX/GRGnR8TpwIuBp4DrgUuAzRFxCrA5ny5ELW1dPuogMDPrZr5ODZ0L\n3BMR9wEXAJvy+ZuAC4vaaC3Jds/PLTYz626+guAi4Mr8/dqI2JW/fwhYW9RGa4l7BGZmMyk8CCQN\nAm8C/nH6Z5ENAtTxt7SkDZJGJY2OjY0d1rZdIzAzm9l89AheD9weEQ/n0w9LOg4gf93d6UsRsTEi\nRiJiZHh4+LA27BqBmdnM5iMI3sHB00IANwLr8/frgRuK2rBrBGZmMys0CCQtBV4LXNc2++PAayXt\nAF6TTxei5lNDZmYzqhW58ojYB6yZNu83ZFcRFS71DWVmZjMq953FeY1g0kFgZtZVqYMgdY3AzGxG\npQ4C1wjMzGZW7iBIXSMwM5tJuYPAdxabmc2o1EHQqhH4eQRmZt2VOghcIzAzm1m5g8A1AjOzGZU6\nCFLXCMzMZlTqIGiNNVRvuEZgZtZNqYPAPQIzs5mVOggGXCMwM5tRqYPAPQIzs5mVOggO1ggcBGZm\n3ZQ6CPIOgQedMzProdRBIImBVD41ZGbWQ9FPKFsp6VpJP5e0XdLLJK2WdJOkHfnrqiLbkCZysdjM\nrIeiewSfA74dES8ATgO2A5cAmyPiFGBzPl2YWpK4R2Bm1kNhQSDpGOCVwGUAETEREY8DFwCb8sU2\nARcW1QbIegS+oczMrLsiewQnA2PAFZK2Sro0f5j92ojYlS/zELC2wDa4RmBmNoMig6AGnAl8MSLO\nAPYx7TRQRATQ8be0pA2SRiWNjo2NHXYjXCMwM+utyCB4EHgwIm7Jp68lC4aHJR0HkL/u7vTliNgY\nESMRMTI8PHzYjXCNwMyst8KCICIeAh6Q9Px81rnAXcCNwPp83nrghqLaAK4RmJnNpFbw+v8Y+Kqk\nQeBe4N1k4XONpIuB+4C3FdmAWuIagZlZL4UGQURsA0Y6fHRukdttV0tdIzAz66XUdxZD9txi9wjM\nzLorfRDUXCMwM+up9EGQukZgZtZT6YNgwDUCM7OeSh8E7hGYmfVW+iCoJYl7BGZmPZQ+CHxDmZlZ\nb6UPAt9QZmbWW/mDwMViM7Oeyh8EvqHMzKyn0geBawRmZr2VPghcIzAz6638QeAagZlZT6UPAg86\nZ2bWW+mDwIPOmZn1Vvog8BATZma9lT4IPOicmVlvhT6hTNKvgCeBBlCPiBFJq4GrgXXAr4C3RcRj\nRbXBNQIzs97mo0fw6og4PSJaj6y8BNgcEacAm/PpwtQS9wjMzHpZiFNDFwCb8vebgAuL3FiaB0GE\nw8DMrJOigyCAf5W0RdKGfN7aiNiVv38IWNvpi5I2SBqVNDo2NnbYDaglAvDpITOzLgqtEQCviIid\nkp4F3CTp5+0fRkRI6vgbOiI2AhsBRkZGDvu3eC3Nsq7RDAbSw12LmVl5zdgjkLRO0mD+/hWS/kDS\nin5WHhE789fdwPXAWcDDko7L13ccsPtwG98P9wjMzHrr59TQ14GQ9FvAFcApwNdm+pKkpZKWt94D\nrwPuBG4E1ueLrQduOIx29y1tBYFvKjMz66ifU0PNiJiU9Gbg7yLi85K29vG9tcD1klrb+VpEfFvS\nbcA1ki4G7gPedriN70ctdY/AzKyXfoKgLumtwLs4eIXPwExfioh7gdM6zP8NcO4zaeRs1JKDNQIz\nMztUP6eG3gO8GvibiLhX0snAlcU2a+64RmBm1tuMPYKIuBP4AwBJxwCLI+Kvi27YXHGNwMyst36u\nGtosaYWkVcA24MuSPll80+aGawRmZr31c2podUTsAd4MfCUiXgz8brHNmjutHoFrBGZmnfUTBDVJ\nw8BbgW8U3J451yoW1xsOAjOzTvoJgr8G/h14ICJulfRc4JfFNmvu1NwjMDPrqZ9i8VXAVW3T95IN\nHHdUSPMawWTTxWIzs076KRYfL+kaSbvyn6slHT8fjZsL7hGYmfXWz6mhK4CbyB4ksy5/f0VxTZpb\nrhGYmfXWTxCsjYi/j4jx/OdSugwdfSRqXT7qHoGZWWf9BMGjki7SQW8HHi26YXOldfmoawRmZp31\nO8TE7wGPAGNkYw69p8hGzaWpGoFPDZmZddTPVUO/As4vvinFmKoR+NSQmVlHXYNA0mfIHjXZUUR8\nsJAWzTHXCMzMeuvVI7hz3lpRoKlB51wjMDPrqGsQRMRl89mQokwNQ+0agZlZR/0Ui2dFUippq6Rv\n5tMnS7pF0t35zWmDRW7fg86ZmfVWeBAA7wO2t01/AvhMRDwPeAy4uMiND6QuFpuZ9VJoEEg6EXgD\ncGk+LeAc4Np8kU0cfPxlIQ72CFwjMDPrZMbLRyUdS3bfwLr25SNiQx/r/yzwYWB5Pr0GeDwi6vn0\ng8AJz6C9z1irRjDpGoGZWUf9PLz+BuDHwA+ARr8rlvRGYHdEbJH0qmfaMEkbgA0AJ5100jP9+hTX\nCMzMeusnCJZGxIcOY90vB94k6XxgEbAC+BywUlIt7xWcCOzs9OWI2AhsBBgZGTns3+KuEZiZ9dZP\njeBfJL3uma44Iv4sIk6MiHXARcB3I+KdwM3AW/LF1pP1OArjGoGZWW/9BMHvA9+WtFfSo5IekzSb\nQec+AnxQ0t1kNYNC71dI5RqBmVkv/ZwaOna2G4mI7wHfy9/fC5w123X2K0lEItcIzMy66TXW0CkR\nsQP47S6L3FFMk+ZeLU1cIzAz66JXj+ASspu9vtDhswBeWUiLClBL5BqBmVkXvcYaujh//Z35a04x\n0kTuEZiZddFPjQBJLwBOJbsMFICI+FpRjZprtUQedM7MrIt+7iz+S+B1wAuA7wC/S3Zz2VETBGni\nGoGZWTf9XD76duDVwK6IeBdwGrC00FbNsYHUNQIzs276CYL9EdEA6pKWAw8Bzym2WXPLNQIzs+76\nqRFslbQSuBwYBfYAtxbaqjnmGoGZWXc9gyAfNvpjEfE48AVJ3wFWRMTt89K6OZIm8g1lZmZd9AyC\niAhJNwEvzKfvnpdWzbGBNPEzi83MuuinRrBN0hmFt6RA7hGYmXXXa4iJ1lDRZwC3SboH2AeIrLNw\n5jy1cdZqiTzonJlZF71ODd0KnAm8aZ7aUhj3CMzMuusVBAKIiHvmqS2FqSWuEZiZddMrCIYlfbDb\nhxHx6QLaU4haKiYbDgIzs056BUEKLCPvGRzN0kTsn/SpITOzTnoFwa6I+F+Hu2JJi4DvA0P5dq6N\niI9KOhm4iuzpZFuAd0XExOFupx++oczMrLtel4/OticwDpwTEacBpwPnSTob+ATwmYh4HvAY2TMP\nCuVB58zMuusVBOfOZsWR2ZtPDuQ/AZwDXJvP3wRcOJvt9MODzpmZddc1CCJiNg+oB0BSKmkbsBu4\nCbgHeDy/PwHgQeCE2W5nJh50zsysu37uLD5sEdGIiNOBE8keWP+Cfr8raYOkUUmjY2Njs2qHawRm\nZt0VGgQt+aB1NwMvA1ZKahWpTwR2dvnOxogYiYiR4eHhWW0/TRLfUGZm1kVhQSBpOB++GkmLgdcC\n28kC4S35YuuBG4pqQ8tAKt9QZmbWRV/PLD5MxwGbJKVkgXNNRHxT0l3AVZL+D7AVuKzANgAeYsLM\nrJfCgiAi7iAbsG76/HvJ6gXzxoPOmZl1Ny81goXmGoGZWXeVCIKaawRmZl1VIwhcIzAz66oyQeAb\nyszMOqtEEKRJQgTuFZiZdVCJIKil2fh5rhOYmR2qGkGQZEHgHoGZ2aEqEQRp0uoROAjMzKarRBC0\negQeeM7M7FCVCII0zXbTNQIzs0NVIghcIzAz665SQeBTQ2Zmh6pGEKTuEZiZdVOJIEgT1wjMzLqp\nRBDUfPmomVlX1QoC1wjMzA5RjSBwjcDMrKsin1n8bEk3S7pL0s8kvS+fv1rSTZJ25K+rimpDi2sE\nZmbdFdkjqAMfiohTgbOBP5R0KnAJsDkiTgE259OF8qkhM7PuCguCiNgVEbfn758EtgMnABcAm/LF\nNgEXFtWGFt9QZmbW3bzUCCStI3uQ/S3A2ojYlX/0ELC2y3c2SBqVNDo2Njar7R8chtpBYGY2XeFB\nIGkZ8E/A+yNiT/tnERFAx9/OEbExIkYiYmR4eHhWbXCNwMysu0KDQNIAWQh8NSKuy2c/LOm4/PPj\ngN1FtgFcIzAz66XIq4YEXAZsj4hPt310I7A+f78euKGoNrSkrhGYmXVVK3DdLwfeBfxU0rZ83p8D\nHweukXQxcB/wtgLbAMCAawRmZl0VFgQR8QNAXT4+t6jtdtKqEbhHYGZ2qGrcWZyfGppsuFhsZjZd\nJYLANQIzs+4qEQS+j8DMrLtqBIFrBGZmXVUiCFLXCMzMuqpEEHisITOz7ioRBKmfUGZm1lUlgmAg\ndY3AzKybSgRB3iGg7hqBmdkhKhEEkqgl8qkhM7MOKhEEkNUJfGrIzOxQlQmCgTRxj8DMrIPKBIF7\nBGZmnVUmCGqJfEOZmVkHlQkC9wjMzDqrTBC4RmBm1lmRj6q8XNJuSXe2zVst6SZJO/LXVUVtfzr3\nCMzMOiuyR/Al4Lxp8y4BNkfEKcDmfHpeuEZgZtZZYUEQEd8HHp02+wJgU/5+E3BhUdufzj0CM7PO\n5rtGsDYiduXvHwLWdltQ0gZJo5JGx8bGZr3h1HcWm5l1tGDF4ogIoOtv5ojYGBEjETEyPDw86+0N\npIl7BGZmHcx3EDws6TiA/HX3fG04dY3AzKyj+Q6CG4H1+fv1wA3zteGaawRmZh0VefnolcCPgOdL\nelDSxcDHgddK2gG8Jp+eF64RmJl1VitqxRHxji4fnVvUNnsZSBP2TzYWYtNmZke0ytxZ7B6BmVln\nlQmCWiI/oczMrIPKBIFvKDMz66wyQeBB58zMOqtMELhHYGbWWWWCYO2KIe77zT6+8ZNfL3RTzMyO\nKIVdPnqkef9r/is/eeAJ3n/1NhKJN7zouIVukpnZEaEyPYKlQzWuePdLOPOklbz3qq3csG0n2XBH\nZmbVVpkggFYYnMUZz17J+67axvmf/wFX3Xo/+yd8o5mZVZeOhr+KR0ZGYnR0dM7Wd2CywfVbd7Lp\nh7/i5w89yZLBlNOfvZIXP2cVZ560ilOPX8Gzlg8hac62aWY23yRtiYiRGZerYhC0RAS3/vJR/vmn\nu7j9/se469d7aF1YtHrpIM9fu5wTVi3m+GMW8V+OWcwJqxZzwsrsZ/FgOuftMTObS/0GQWWKxZ1I\n4qXPXcNLn7sGgH3jde7c+QTbd+1h+64n+cXuJ/nBjkfY/eQBpl95unyoxpplg6xZNsTyRTWWDtVY\nNpjNW7tiEc9aPsQxiwdYPJiydKjGqiWDrFk6SJK4l2FmR5ZKB8F0S4dqTwuGlslGk91PjrPzsf38\n+vH97Hx8P4/sHec3eyf4zb5xHt03wf2PPsXeA3Ue3TfR9ca1WiKG84BYNJCyeCALiRWLa6xYNMDy\nRTUWD6YsGUhZMlRj+VCN5YsGWLaoxqKBhMUDKYsHU5YPDbBoIPGpKzObEw6CPgykydQpoZk0m8Gj\nT03w8J4DPHmgzv6JBvsmsoB4eM8BHnpinD0HJjkw2eDAZIOdj+9n+65J9uyfZO9EnX7P1NUSsWQw\nZWggZTBNGKolLFtUY9lQ9rNoIGWoljBYS6glIk0Samn2ndYytTRBgARDtZQlQ1kIDQ2kDKRiME2o\npQkDqRjIt9Far0PIrDwcBHMsScSxy4Y4dtnQM/5uRDBeb7J/osHe8Tp7x+s8eaDOvvE6+ycb7J9o\n8NREnb3jDfaOT7L3QJ2JRjBRbzJez79zoM79+55ivN5kfLLBeL1JvRk0mkG92eTA5OwH3suCIwuF\nRbU0C5tUU4EzkIo0yaZreQBND5Js+WRquTQRtTQ5+D4Rg7WEoVq2fNp2Si1p/07+OpBmy6SJSJS9\nphIS1FIxlLdzoLVdiTQViSCR8p/svYSDzirFQXAEkZT9ch1IWbV0sJBtNJrBvoksMBrNIAKCLID2\njdfZN95gotFgoh5MNprUm00mG9n78ckmB+oNDkw02J+HzIH8tZGHzWQjaEZQbwb1RpN6IzhQbzDZ\nyEJovN7gwOTB5euN9qA6ci5cSNuCphUOibLAGUzFQC3rTbUkEkkePlkwtb6fkCRMBVQrpNqD52nb\nSjR1TICp9bXCrxWszWbQzJcbTJM85BKSPMSm55hoC7229qX5glK2TP5fvlwrGNsDM1vbwXVl20uV\nty9NSBNoRtY7jqntM7We1mtrX2qJnha87W2fHtCtf8NOsgDP/03z5dTah7Z9VNu/e6eSXRX/CFiQ\nIJB0HvA5IAUujYh5e1JZ1aWJWLFogBWLBha6KYeIyH65TebhMFlvZj2bemOqWN9aJgud5lToTE03\nod7M5jeb0Iig3og83JpMNIJms/WdJhHZMs08FLN1N7PvNYNGI/tl1syXmczbNTFtSPOp77WFWr2R\n9cKaTaba2mwGjQgazWxfIlrtzZdpxtN+ETUjC9b2dTaaMfXLEWCikbXnKLgA8KjRHiitgIG2QG3r\nfXb8/rR1TYUgTw9pTS3z9IBS2/cuX/8STlqzpIjdnDLvQSApBb4AvBZ4ELhN0o0Rcdd8t8WOLNlf\nlpAm+aW5z/zsWmVFHhatXkKnUGgFSrMJk82st9ZaNjgYTK1ls2CMtvdZALe+08xDuRVwrUDOgir/\na1vA1PqztrV6C60wnmwc7DlMv5y9tZ323mujySE9nvZ1x1TgHtynZsRUG7L9OfgHRfu62v8tpvYv\n/yvk4Hzyf+vO/84H96b1b5mvN19n+7aetgzT2p1vb7BW/H2/C9EjOAu4OyLuBZB0FXAB4CAwO0yS\nqKXVO6Vhc2Mhhpg4AXigbfrBfJ6ZmS2AI3asIUkbJI1KGh0bG1vo5piZldZCBMFO4Nlt0yfm854m\nIjZGxEhEjAwPD89b48zMqmYhguA24BRJJ0saBC4CblyAdpiZGQtQLI6IuqQ/Ar5Ddvno5RHxs/lu\nh5mZZRbkPoKI+BbwrYXYtpmZPd0RWyw2M7P54SAwM6u4o+LBNJLGgPsO8+vHAo/MYXOOFlXc7yru\nM1Rzv73P/XlORMx42eVREQSzIWm0nyf0lE0V97uK+wzV3G/v89zyqSEzs4pzEJiZVVwVgmDjQjdg\ngVRxv6u4z1DN/fY+z6HS1wjMzKy3KvQIzMysh1IHgaTzJP2npLslXbLQ7SmCpGdLulnSXZJ+Jul9\n+fzVkm6StCN/XbXQbZ1rklJJWyV9M58+WdIt+fG+Oh/LqlQkrZR0raSfS9ou6WVlP9aSPpD/v32n\npCslLSrjsZZ0uaTdku5sm9fx2Crz+Xz/75B05my2XdogaHsS2uuBU4F3SDp1YVtViDrwoYg4FTgb\n+MN8Py8BNkfEKcDmfLps3gdsb5v+BPCZiHge8Bhw8YK0qlifA74dES8ATiPb/9Iea0knAO8FRiLi\nhWTjk11EOY/1l4Dzps3rdmxfD5yS/2wAvjibDZc2CGh7ElpETACtJ6GVSkTsiojb8/dPkv1iOIFs\nXzfli20CLlyYFhZD0onAG4BL82kB5wDX5ouUcZ+PAV4JXAYQERMR8TglP9ZkY6ItllQDlgC7KOGx\njojvA49Om93t2F4A/ENkfgyslHTc4W67zEFQuSehSVoHnAHcAqyNiF35Rw8BaxeoWUX5LPBhoPUU\n+TXA4xFRz6fLeLxPBsaAK/JTYpdKWkqJj3VE7AQ+BdxPFgBPAFso/7Fu6XZs5/T3W5mDoFIkLQP+\nCXh/ROxp/yyyS8NKc3mYpDcCuyNiy0K3ZZ7VgDOBL0bEGcA+pp0GKuGxXkX21+/JwPHAUg49fVIJ\nRR7bMgdBX09CKwNJA2Qh8NWIuC6f/XCrq5i/7l6o9hXg5cCbJP2K7JTfOWTnzlfmpw+gnMf7QeDB\niLgln76WLBjKfKxfA/wyIsYiYhK4juz4l/1Yt3Q7tnP6+63MQVCJJ6Hl58YvA7ZHxKfbProRWJ+/\nXw/cMN9tK0pE/FlEnBgR68iO63cj4p3AzcBb8sVKtc8AEfEQ8ICk5+ezzgXuosTHmuyU0NmSluT/\nr7f2udTHuk23Y3sj8Hv51UNnA0+0nUJ65iKitD/A+cAvgHuAv1jo9hS0j68g6y7eAWzLf84nO2e+\nGdgB/BuweqHbWtD+vwr4Zv7+ucCtwN3APwJDC92+Avb3dGA0P95fB1aV/VgDfwX8HLgT+DIwVMZj\nDVxJVgeZJOv9Xdzt2AIiuyryHuCnZFdVHfa2fWexmVnFlfnUkJmZ9cFBYGZWcQ4CM7OKcxCYmVWc\ng8DMrOIcBHbUkxSS/rZt+k8kfWwO1jsk6d8kbZP09mmffUnSL/PPtkn64Wy3N23935NUqWfy2sKp\nzbyI2RFvHHizpP8XEY/M4XrPAIiI07t8/qcRcW2Xz8yOGu4RWBnUyR7j94HpH0haJ+m7+ZjtmyWd\n1GGZ1ZK+ni/zY0kvkvQs4CvAS/K/+H+rn4ZI+pikL0v6UT6G/P/M50vSJ/Mx9X/a3sOQ9JF83k8k\nfbxtdW+VdKukX0j6nXzZ387nbcvbe8oz+pcy68A9AiuLLwB3SPqbafP/DtgUEZskvQf4PIcOWfxX\nwNaIuFDSOWTD+54u6X8AfxIRb+yyzU9K+sv8/c8iG+YC4EVkz4ZYCmyV9M/Ay8juCj4NOBa4TdL3\n83kXAC+NiKckrW5bfy0izpJ0PvBRsnF3fh/4XER8NR86Je37X8isCweBlUJE7JH0D2QPMdnf9tHL\ngDfn778MTA8KyIbp+O/5er4raY2kFX1sttupoRsiYj+wX9LNZM/GeAVwZUQ0yAYS+3fgJcB/A66I\niKfy7bePR98aQHALsC5//yPgL/LnMVwXETv6aKdZTz41ZGXyWbLxWZYucDumj9tyuOO4jOevDfI/\n2iLia8CbyMLuW3kPxmxWHARWGvlf09fw9McW/pBshFKAdwL/0eGr/5F/hqRXAY/EtGc6PEMXKHuu\n7hqyQfFuy7fxdmXPWR4me9LYrcBNwLslLcm3v7rLOsk/fy5wb0R8nmwkyhfNop1mgE8NWfn8LfBH\nbdN/TPZErz8le7rXuzt852PA5ZLuAJ7i4LC/M2mvEUB2CgiykUFvJqsF/O+I+LWk68lOU/2ErIfw\n4ciGlf62pNOBUUkTwLeAP++xzbcB75I0SfbEqv/bZ1vNuvLoo2ZzKL9/YW9EfGqh22LWL58aMjOr\nOPcIzMwqzj0CM7OKcxCYmVWcg8DMrOIcBGZmFecgMDOrOAeBmVnF/X9OoxHMkKg1YwAAAABJRU5E\nrkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmNprnGg3-69",
        "colab_type": "code",
        "outputId": "a4ed85bb-b170-4806-e132-d9adae20118c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "df=pd.DataFrame({'x': range(0,818,5), '4 pixel': acc, '5 pixel': acc_1, '6 pixel': acc_2 })\n",
        "\n",
        "\n",
        "plt.plot( 'x', '4 pixel', data=df, marker='.', markerfacecolor='blue', markersize=12, color='skyblue', linewidth=4)\n",
        "plt.plot( 'x', '5 pixel', data=df, marker='', color='olive', linewidth=2)\n",
        "plt.plot( 'x', '6 pixel', data=df, marker='', color='olive', linewidth=2, linestyle='dashed')\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f78308fc160>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nbxiGUUaY0TcMwygjzOgbhmGUEf8HweR9qnI927QAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4hpNEqOpnq-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}