{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CUDA Assignment 4.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "PaD9DeCLPQhj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as dsets\n",
        "from torchvision.utils import make_grid\n",
        "import matplotlib.pyplot as plt\n",
        "from tabulate import tabulate\n",
        "import numpy as np\n",
        "#Loading the cifar-10 dataset\n",
        "\n",
        " \n",
        "train_dataset = dsets.CIFAR10(root='./data', \n",
        "                            train=True, \n",
        "                            transform=transforms.ToTensor(),\n",
        "                            download=True)\n",
        " \n",
        "test_dataset = dsets.CIFAR10(root='./data', \n",
        "                           train=False, \n",
        "                           transform=transforms.ToTensor())\n",
        "\n",
        "comparison_array = [['Pooling type','Accuracy']]\n",
        " \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dFEaMxKr-Jt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Making Datasets Iterable with batch size of 100\n",
        " \n",
        "batch_size = 100\n",
        "n_iters = 6000\n",
        "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        " \n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        " \n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)\n",
        " \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80V2JebvsEsa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Network Structure with Max Pooling in Pooling layer\n",
        "\n",
        "'''\n",
        "STEP 3: CREATE MODEL CLASS\n",
        "'''\n",
        "class CNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNModel, self).__init__()\n",
        "         \n",
        "        # Convolution 1\n",
        "        self.cnn1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        #Batch Normalization \n",
        "        self.batchnorm1=nn.BatchNorm2d(32)\n",
        "                 \n",
        "                      \n",
        "        # Convolution 2\n",
        "        self.cnn2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        #Batch Normalization \n",
        "        self.batchnorm2=nn.BatchNorm2d(32)\n",
        "        # Max pool 1\n",
        "        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
        "        #Dropout 1\n",
        "        self.fc1_drop = nn.Dropout(0.2)\n",
        "        \n",
        "        \n",
        "        # Convolution 3\n",
        "        self.cnn3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        #Batch Normalization \n",
        "        self.batchnorm3=nn.BatchNorm2d(64)\n",
        "        \n",
        "        # Convolution 4\n",
        "        self.cnn4 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu4 = nn.ReLU()\n",
        "        #Batch Normalization \n",
        "        self.batchnorm4=nn.BatchNorm2d(64)\n",
        "        # Max pool 2\n",
        "        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
        "        #Dropout 2\n",
        "        self.fc2_drop = nn.Dropout(0.2)\n",
        "        \n",
        "        # Convolution 5\n",
        "        self.cnn5 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu5 = nn.ReLU()\n",
        "        #Batch Normalization \n",
        "        self.batchnorm5=nn.BatchNorm2d(128)\n",
        "        \n",
        "        # Convolution 6\n",
        "        self.cnn6 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu6 = nn.ReLU()\n",
        "        #Batch Normalization \n",
        "        self.batchnorm6=nn.BatchNorm2d(128)\n",
        "        #Max pool 3        \n",
        "        self.maxpool3 = nn.MaxPool2d(kernel_size=2)\n",
        "        #Dropout 3\n",
        "        self.fc3_drop = nn.Dropout(0.2)\n",
        "               \n",
        "        \n",
        "         \n",
        "        #Fully connected 1 (readout)\n",
        "        self.fc1 = nn.Linear(128*4*4, 10) \n",
        "    \n",
        "    def forward(self, x):\n",
        "        # x =  torch.Size([100, 3, 32, 32])\n",
        "        # Convolution 1\n",
        "        out = self.cnn1(x) #torch.Size([100, 32, 32, 32])\n",
        "        out = self.relu1(out) #torch.Size([100,32, 32, 32])\n",
        "        out = self.batchnorm1(out)\n",
        "        # Convolution 2\n",
        "        out = self.cnn2(out) #torch.Size([100, 32, 32, 32])\n",
        "        out = self.relu2(out) #torch.Size([100, 32, 32, 32])\n",
        "        out=self.batchnorm2(out)\n",
        "        # Max pool 1\n",
        "        out = self.maxpool1(out) #torch.Size([100, 32, 16, 16])\n",
        "        out = self.fc1_drop(out)\n",
        "        \n",
        "        # Convolution 3\n",
        "        out = self.cnn3(out) #torch.Size([100, 64, 16, 16])\n",
        "        out = self.relu3(out) #torch.Size([100, 64, 16, 16])\n",
        "        out=self.batchnorm3(out)\n",
        "        \n",
        "        # Convolution 4\n",
        "        out = self.cnn4(out) #torch.Size([100, 64, 16, 16])\n",
        "        out = self.relu4(out) #torch.Size([100, 64, 16, 16])\n",
        "        out=self.batchnorm4(out)\n",
        "        # Max pool 2\n",
        "        out = self.maxpool2(out) #torch.Size([100, 64, 8, 8])\n",
        "        out = self.fc2_drop(out)\n",
        "         \n",
        "        \n",
        "         # Convolution 5\n",
        "        out = self.cnn5(out) #torch.Size([100, 128, 8, 8])\n",
        "        out = self.relu5(out) #torch.Size([100, 128, 8, 8])\n",
        "        out=self.batchnorm5(out)\n",
        "        \n",
        "         # Convolution 6\n",
        "        out = self.cnn6(out) #torch.Size([100, 128, 8, 8])\n",
        "        out = self.relu6(out) #torch.Size([100, 128, 8, 8])\n",
        "        out=self.batchnorm6(out)\n",
        "        # Max pool 3\n",
        "        out = self.maxpool3(out) #torch.Size([100,128, 4, 4])\n",
        "        out = self.fc3_drop(out)\n",
        "         \n",
        "         \n",
        "\n",
        "        out = out.view(out.size(0), -1)   \n",
        " \n",
        "        # Linear function (readout)\n",
        "        out = self.fc1(out)\n",
        "         \n",
        "        return out\n",
        "      \n",
        "     "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1uEos_9eif0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Instantiate Model Class\n",
        " \n",
        "model = CNNModel()\n",
        " \n",
        "if torch.cuda.is_available():\n",
        "    model.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sa4LvYXyeicI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Instantiate Loss Class\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lzOAf3keiYC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#Instantiate Optimizer Class\n",
        "learning_rate = 0.01\n",
        " \n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TuwlYa_eiTe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Training the Model\n",
        "iter = 0\n",
        "arr=['MAX Pooling']\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "         \n",
        "        if torch.cuda.is_available():\n",
        "            images = images.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "         \n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "         \n",
        "        # Forward pass to get output/logits\n",
        "        outputs = model(images)\n",
        "         \n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(outputs, labels)\n",
        "         \n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "         \n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "         \n",
        "        iter += 1\n",
        "         \n",
        "        if iter % 500 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for images, labels in test_loader:\n",
        "                \n",
        "                if torch.cuda.is_available():\n",
        "                    images = images.cuda()\n",
        "                 \n",
        "                # Forward pass only to get logits/output\n",
        "                outputs = model(images)\n",
        "                 \n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                 \n",
        "                # Total number of labels\n",
        "                total += labels.size(0)\n",
        "                 \n",
        "                # Total correct predictions\n",
        "                correct += (predicted.cpu() == labels.cpu()).sum().float()\n",
        "\n",
        "             \n",
        "            accuracy = 100. * correct / total\n",
        "             \n",
        "            # Print Loss\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))\n",
        "            \n",
        "arr.append(round(float(accuracy),2))\n",
        "comparison_array.append(arr) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qY40a00mbvqh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#function definition for Visualing the kernels\n",
        "def filter_visualization(kernels,x,y):\n",
        "  kernels = kernels - kernels.min()\n",
        "  kernels = kernels / kernels.max()\n",
        "  fig = plt.figure()\n",
        "  plt.figure(figsize=(y,x))\n",
        "  for idx, filt  in enumerate(kernels):\n",
        "      plt.subplot(x,y, idx + 1)\n",
        "      plt.imshow(filt[0, :, :])\n",
        "      plt.axis('off')\n",
        "    \n",
        "    \n",
        "  fig.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmb7usWyChVt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Visualizing the 1st CNN layer kernels\n",
        "print('Visualization of 1st CNN layer Kernels')\n",
        "kernels = model.cnn1.weight.cpu().detach().clone()\n",
        "filter_visualization(kernels,4,8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHIq637thFsg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Visualizing the 2nd CNN layer kernels\n",
        "print('Visualization of 2nd CNN layer Kernels')\n",
        "kernels = model.cnn2.weight.cpu().detach().clone()\n",
        "filter_visualization(kernels,4,8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzyaJjdqhFom",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Visualizing the 3rd CNN layer kernels\n",
        "print('Visualization of 3rd CNN layer Kernels')\n",
        "kernels = model.cnn3.weight.cpu().detach().clone()\n",
        "filter_visualization(kernels,4,16)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQ8x1zCWhFju",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Visualizing the 4th CNN layer kernels\n",
        "print('Visualization of 4th CNN layer Kernels')\n",
        "kernels = model.cnn4.weight.cpu().detach().clone()\n",
        "filter_visualization(kernels,4,16)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3riFMfw3hFfH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Visualizing the 5th CNN layer kernels\n",
        "print('Visualization of 5th CNN layer Kernels')\n",
        "kernels = model.cnn5.weight.cpu().detach().clone()\n",
        "filter_visualization(kernels,8,16)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8R1HzOqhFZr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Visualizing the 6th CNN layer kernels\n",
        "print('Visualization of 6th CNN layer Kernels')\n",
        "kernels = model.cnn6.weight.cpu().detach().clone()\n",
        "filter_visualization(kernels,8,16)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgAEnTAz1WWP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#function definition for visualizing activation map\n",
        "def Activation_Map_Plot(tensor,r,c):\n",
        "  tensor=tensor.squeeze()\n",
        "  fig = plt.figure(figsize=(c,r))\n",
        "  for i in range(r*c):\n",
        "    ax1 = fig.add_subplot(r,c,i+1)\n",
        "    ax1.imshow(tensor[i])\n",
        "    ax1.axis('off')\n",
        "    ax1.set_xticklabels([])\n",
        "    ax1.set_yticklabels([])\n",
        "    \n",
        "  plt.subplots_adjust(wspace=0.3, hspace=0.3)\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqwki1OI1mrc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Activation maps of different layers\n",
        "from torch.autograd import Variable\n",
        "for images, labels in test_loader:\n",
        "  images = Variable(images.view(-1, 3*32*32))\n",
        "  image = images[0].view(1, 3, 32, 32).cuda()\n",
        "      \n",
        "#1st CNN layer\n",
        "out1 = model.cnn1(image)\n",
        "print('Activation map of Conv layer 1')\n",
        "Activation_Map_Plot(out1.data.cpu().numpy(),2,16)\n",
        "\n",
        "#2nd CNN layer\n",
        "out2 = model.cnn2(out1)\n",
        "print('Activation map of Conv layer 2')\n",
        "Activation_Map_Plot(out2.data.cpu().numpy(),2,16)\n",
        "\n",
        "#3rd CNN layer\n",
        "out3 = model.cnn3(out2)\n",
        "print('Activation map of Conv layer 3')\n",
        "Activation_Map_Plot(out3.data.cpu().numpy(),8,8)\n",
        "  \n",
        "#4th CNN layer\n",
        "out4 = model.cnn4(out3)\n",
        "print('Activation map of Conv layer 4')\n",
        "Activation_Map_Plot(out4.data.cpu().numpy(),8,8)\n",
        "\n",
        "#5th CNN layer\n",
        "out5 = model.cnn5(out4)\n",
        "print('Activation map of Conv layer 5')\n",
        "Activation_Map_Plot(out5.data.cpu().numpy(),8,16)\n",
        "\n",
        "#6th CNN layer\n",
        "out6 = model.cnn6(out5)\n",
        "print('Activation map of Conv layer 6')\n",
        "Activation_Map_Plot(out6.data.cpu().numpy(),8,16)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGwo3iubPXyh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Network Structure with Average Pooling in Pooling layer\n",
        "class CNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNModel, self).__init__()\n",
        "         \n",
        "        # Convolution 1\n",
        "        self.cnn1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        #Batch Normalization 1\n",
        "        self.batchnorm1=nn.BatchNorm2d(32)\n",
        "                 \n",
        "            \n",
        "        # Convolution 2\n",
        "        self.cnn2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        #Batch Normalization 2\n",
        "        self.batchnorm2=nn.BatchNorm2d(32)\n",
        "        # Average pool 1\n",
        "        self.avgpool1 = nn.AvgPool2d(kernel_size=2)\n",
        "        #Dropout 1\n",
        "        self.fc1_drop = nn.Dropout(0.2)\n",
        "        \n",
        "        # Convolution 3\n",
        "        self.cnn3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        #Batch Normalization 3\n",
        "        self.batchnorm3=nn.BatchNorm2d(64)\n",
        "        \n",
        "        # Convolution 4\n",
        "        self.cnn4 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu4 = nn.ReLU()\n",
        "        #Batch Normalization 4\n",
        "        self.batchnorm4=nn.BatchNorm2d(64)\n",
        "        # Average pool 2\n",
        "        self.avgpool2 = nn.AvgPool2d(kernel_size=2)\n",
        "        #Dropout 2\n",
        "        self.fc2_drop = nn.Dropout(0.2)\n",
        "        \n",
        "        # Convolution 5\n",
        "        self.cnn5 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu5 = nn.ReLU()\n",
        "        #Batch Normalization 5\n",
        "        self.batchnorm5=nn.BatchNorm2d(128)\n",
        "        \n",
        "        # Convolution 6\n",
        "        self.cnn6 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu6 = nn.ReLU()\n",
        "        #Batch Normalization 6\n",
        "        self.batchnorm6=nn.BatchNorm2d(128)\n",
        "        \n",
        "        # Average pool 3\n",
        "        self.avgpool3 = nn.AvgPool2d(kernel_size=2)\n",
        "        #Dropout 3\n",
        "        self.fc3_drop = nn.Dropout(0.2)\n",
        "        \n",
        "         \n",
        "        # Fully connected 1 (readout)\n",
        "        self.fc1 = nn.Linear(128*4*4, 10) \n",
        "    \n",
        "    def forward(self, x):\n",
        "        # x =  torch.Size([100, 3, 32, 32])\n",
        "        # Convolution 1\n",
        "        out = self.cnn1(x) #torch.Size([100, 32, 32, 32])\n",
        "        \n",
        "        out = self.relu1(out) #torch.Size([100, 32, 32, 32])\n",
        "        out = self.batchnorm1(out)\n",
        "        # Convolution 2\n",
        "        out = self.cnn2(out) #torch.Size([100, 32, 32, 32])\n",
        "        out = self.relu2(out) #torch.Size([100, 32, 32, 32])\n",
        "        out=self.batchnorm2(out)\n",
        "        # Average pool 1\n",
        "        out = self.avgpool1(out) #torch.Size([100, 32, 16, 16])\n",
        "        out = self.fc1_drop(out)\n",
        "        \n",
        "        # Convolution 3\n",
        "        out = self.cnn3(out) #torch.Size([100, 64, 16, 16])\n",
        "        out = self.relu3(out) #torch.Size([100, 64, 16, 16])\n",
        "        out=self.batchnorm3(out)\n",
        "        \n",
        "        # Convolution 4\n",
        "        out = self.cnn4(out) #torch.Size([100, 64, 16, 16])\n",
        "        out = self.relu4(out) #torch.Size([100, 64, 16, 16])\n",
        "        out=self.batchnorm4(out)\n",
        "        # Average pool 2\n",
        "        out = self.avgpool2(out) #torch.Size([100, 64, 8, 8])\n",
        "        out = self.fc2_drop(out)\n",
        "         \n",
        "        \n",
        "         # Convolution 5\n",
        "        out = self.cnn5(out) #torch.Size([100, 128, 8, 8])\n",
        "        out = self.relu5(out) #torch.Size([100, 128, 8, 8])\n",
        "        out=self.batchnorm5(out)\n",
        "        \n",
        "         # Convolution 6\n",
        "        out = self.cnn6(out) #torch.Size([100, 128, 8, 8])\n",
        "        out = self.relu6(out) #torch.Size([100, 128, 8, 8])\n",
        "        out=self.batchnorm6(out)\n",
        "        # Average pool 3\n",
        "        out = self.avgpool3(out) #torch.Size([100, 128, 4, 4])\n",
        "        out = self.fc3_drop(out)\n",
        "        \n",
        "         \n",
        "\n",
        "        out = out.view(out.size(0), -1)   \n",
        " \n",
        "        # Linear function (readout)\n",
        "        out = self.fc1(out)\n",
        "        \n",
        "    \n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwCFuoNybATw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Training the Network with average pooling\n",
        "iter = 0\n",
        "arr=['AVG Pooling']\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "         \n",
        "        \n",
        "        if torch.cuda.is_available():\n",
        "            images = images.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "         \n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "         \n",
        "        # Forward pass to get output/logits\n",
        "        outputs = model(images)\n",
        "         \n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(outputs, labels)\n",
        "         \n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "         \n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "         \n",
        "        iter += 1\n",
        "        \n",
        "        if iter % 500 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for images, labels in test_loader:\n",
        "                \n",
        "                if torch.cuda.is_available():\n",
        "                    images = images.cuda()\n",
        "                 \n",
        "                # Forward pass only to get logits/output\n",
        "                outputs = model(images)\n",
        "                 \n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                 \n",
        "                # Total number of labels\n",
        "                total += labels.size(0)\n",
        "                 \n",
        "                # Total correct predictions\n",
        "                correct += (predicted.cpu() == labels.cpu()).sum().float()\n",
        "\n",
        "             \n",
        "            accuracy = 100. * correct / total\n",
        "             \n",
        "            # Print Loss\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))\n",
        "arr.append(round(float(accuracy),2))\n",
        "comparison_array.append(arr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5_i6sPMvkiu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(tabulate(comparison_array,\n",
        "       headers=\"firstrow\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slm3t2fJvkfe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if(comparison_array[1][1]>comparison_array[2][1]):\n",
        "   print(str(comparison_array[1][0])+' has better accuracy over '+str(comparison_array[2][0]))\n",
        "else:\n",
        "  \n",
        "   print(str(comparison_array[2][0])+' has better accuracy over '+str(comparison_array[1][0]))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}