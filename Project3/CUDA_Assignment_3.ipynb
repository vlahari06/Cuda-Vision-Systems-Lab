{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyDpyvLIABuc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Importing required libraries\n",
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sn\n",
        "from tabulate import tabulate\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.autograd import Variable\n",
        "\n",
        "cuda = torch.cuda.is_available()\n",
        "\n",
        "torch.manual_seed(42)\n",
        "if cuda:\n",
        "    torch.cuda.manual_seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siWmnTX-AOsu",
        "colab_type": "code",
        "outputId": "422ff466-d6b1-4c2f-e1af-b2d00079da4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Downloading the cifar-10 datasets and normalization\n",
        "batch_size = 100\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.CIFAR10('data', train=True, download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0,), (1,))\n",
        "                   ])),\n",
        "    batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.CIFAR10('data', train=False, transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0,), (1,))\n",
        "                   ])),\n",
        "    batch_size=batch_size, shuffle=True)\n",
        "\n",
        "class_name=['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
        "\n",
        "array = [['Model','Final Accuracy','Convergence time(sec)']]\n",
        "\n",
        "depth_array = [['Number of Layers','Final Accuracy','Convergence time(sec)']]"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUpIL-l4T19Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Network with Droupout regularization implementated using relu and 3 layers\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(3*32*32, 2000)\n",
        "        self.fc1_drop = nn.Dropout(0.2)\n",
        "        self.fc2 = nn.Linear(2000, 200)\n",
        "        self.fc2_drop = nn.Dropout(0.2)\n",
        "        self.fc3 = nn.Linear(200, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 3*32*32)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc1_drop(x)\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc2_drop(x)\n",
        "        return F.log_softmax(self.fc3(x),dim=1)\n",
        "    \n",
        "\n",
        "model = Net()\n",
        "if cuda:\n",
        "    model.cuda()\n",
        "    \n",
        "    \n",
        "#Traning network with dropout regularization\n",
        "def train(epoch, log_interval=1000):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        if cuda:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * float(batch_idx) / len(train_loader), loss.item()))\n",
        "            \n",
        "#Testing network with dropout regularization\n",
        "def test(loss_vector, accuracy_vector):\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    for data, target in test_loader:\n",
        "        if cuda:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        data.requires_grad_(False)\n",
        "        target.requires_grad_(False)\n",
        "        output = model(data)\n",
        "        test_loss += F.nll_loss(output, target).item()\n",
        "        pred = output.data.max(1)[1] \n",
        "        correct += pred.eq(target.data).cpu().sum().float()\n",
        "        for i in range(50):\n",
        "                  confusion_matrix[pred[i]][target[i]]+=1\n",
        "    test_loss /= len(test_loader)\n",
        "    loss_vector.append(test_loss)\n",
        "    \n",
        "    accuracy = 100. * correct / float(len(test_loader.dataset))\n",
        "    accuracy_vector.append(accuracy)\n",
        "    \n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset), float(accuracy)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1LcJdJ3T1sz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#SGD optimizer with Dropout regularization\n",
        "arr=['SGD with Dropout']\n",
        "\n",
        "#creating confusion matrix\n",
        "confusion_matrix=np.array([[0 for x in range(10)] for y in range(10)])\n",
        "confusion_matrix.astype(int)\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
        "epochs = 10\n",
        "\n",
        "lossv, accv = [], []\n",
        "start=time.time()\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train(epoch)\n",
        "    test(lossv, accv)\n",
        "end=time.time()\n",
        "df_cm = pd.DataFrame(confusion_matrix, index = [i for i in class_name],\n",
        "                  columns = [i for i in class_name])\n",
        "\n",
        "#plot confusion matrix for this network \n",
        "plt.figure(figsize = (20,10))\n",
        "x=sn.heatmap(df_cm, annot=True)\n",
        "\n",
        "arr.append(round(float(accv[-1]),2))\n",
        "arr.append(round(float(end-start),2))\n",
        "array.append(arr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owc8ij_OT0lo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ADAM optimizer with Dropout regularization\n",
        "arr=['Adam with Dropout']\n",
        "confusion_matrix=np.array([[0 for x in range(10)] for y in range(10)])\n",
        "confusion_matrix.astype(int)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "epochs = 10\n",
        "\n",
        "lossv, accv = [], []\n",
        "\n",
        "start=time.time()\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train(epoch)\n",
        "    test(lossv, accv)\n",
        "end=time.time()\n",
        "\n",
        "df_cm = pd.DataFrame(confusion_matrix, index = [i for i in class_name],\n",
        "                  columns = [i for i in class_name])\n",
        "#plot confusion matrix for this network \n",
        "plt.figure(figsize = (20,10))\n",
        "x=sn.heatmap(df_cm, annot=True)\n",
        "\n",
        "arr.append(round(float(accv[-1]),2))\n",
        "arr.append(round(float(end-start),2))\n",
        "array.append(arr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jPOvGemT0Wm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Adagrad optimizer with Dropout regularization\n",
        "arr=['Adagrad with Dropout']\n",
        "confusion_matrix=np.array([[0 for x in range(10)] for y in range(10)])\n",
        "confusion_matrix.astype(int)\n",
        "optimizer = optim.Adagrad(model.parameters(), lr=0.01, lr_decay=0.001, weight_decay=0.01, initial_accumulator_value=0)\n",
        "epochs = 10\n",
        "\n",
        "lossv, accv = [], []\n",
        "\n",
        "start=time.time()\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train(epoch)\n",
        "    test(lossv, accv)\n",
        "end=time.time()\n",
        "\n",
        "df_cm = pd.DataFrame(confusion_matrix, index = [i for i in class_name],\n",
        "                  columns = [i for i in class_name])\n",
        "#plot confusion matrix for this network \n",
        "plt.figure(figsize = (20,20))\n",
        "x=sn.heatmap(df_cm, annot=True)\n",
        "\n",
        "arr.append(round(float(accv[-1]),2))\n",
        "arr.append(round(float(end-start),2))\n",
        "array.append(arr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QB0-xX-AT0F_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Adadelta optimizer with Dropout regularization\n",
        "arr=['Adadelta with Dropout']\n",
        "confusion_matrix=np.array([[0 for x in range(10)] for y in range(10)])\n",
        "confusion_matrix.astype(int)\n",
        "optimizer = optim.Adadelta(model.parameters(), lr=1.0, rho=0.9, eps=1e-06, weight_decay=0.1)\n",
        "epochs = 10\n",
        "\n",
        "lossv, accv = [], []\n",
        "start=time.time()\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train(epoch)\n",
        "    test(lossv, accv)\n",
        "end=time.time()\n",
        "df_cm = pd.DataFrame(confusion_matrix, index = [i for i in class_name],\n",
        "                  columns = [i for i in class_name])\n",
        "\n",
        "#plot confusion matrix for this network \n",
        "plt.figure(figsize = (20,10))\n",
        "x=sn.heatmap(df_cm, annot=True)\n",
        "\n",
        "arr.append(round(float(accv[-1]),2))\n",
        "arr.append(round(float(end-start),2))\n",
        "array.append(arr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eb8Re4uxTzWu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#RMSprop optimizer with Dropout regularization\n",
        "arr=['RMSprop with dropout']\n",
        "confusion_matrix=np.array([[0 for x in range(10)] for y in range(10)])\n",
        "confusion_matrix.astype(int)\n",
        "optimizer = optim.RMSprop(model.parameters(), lr=0.01, alpha=0.99, eps=1e-08, weight_decay=0.1, momentum=0.9, centered=True)\n",
        "epochs = 10\n",
        "\n",
        "lossv, accv = [], []\n",
        "\n",
        "start=time.time()\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train(epoch)\n",
        "    test(lossv, accv)\n",
        "end=time.time()\n",
        "\n",
        "df_cm = pd.DataFrame(confusion_matrix, index = [i for i in class_name],\n",
        "                  columns = [i for i in class_name])\n",
        "#plot confusion matrix for this network \n",
        "plt.figure(figsize = (20,10))\n",
        "x=sn.heatmap(df_cm, annot=True)\n",
        "\n",
        "arr.append(round(float(accv[-1]),2))\n",
        "arr.append(round(float(end-start),2))\n",
        "array.append(arr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bmjx2CRHAa5j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Network with L2 regularizer using tanh and sigmoid with 3 layers\n",
        "class Net_L2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net_L2, self).__init__()\n",
        "        self.fc1 = nn.Linear(3*32*32,100)\n",
        "        \n",
        "        self.fc2 = nn.Linear(100, 50)\n",
        "        \n",
        "        self.fc3 = nn.Linear(50, 10)\n",
        "       \n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 3*32*32)\n",
        "        x = torch.tanh(self.fc1(x))\n",
        "        \n",
        "        x = torch.sigmoid(self.fc2(x))\n",
        "        \n",
        "        return F.relu(self.fc3(x))\n",
        "\n",
        "model = Net_L2()\n",
        "if cuda:\n",
        "    model.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfQhXcVoBoAf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#SGD optimizer with L2 regularization\n",
        "arr=['SGD with L2 Regularization']\n",
        "\n",
        "#creating confusion matrix\n",
        "confusion_matrix=np.array([[0 for x in range(10)] for y in range(10)])\n",
        "confusion_matrix.astype(int)\n",
        "\n",
        "#L2 regularizer giver as parameters to SGD using weight_decay\n",
        "optimizer = optim.SGD(model_l2.parameters(), lr=0.001, momentum=0.5,weight_decay=0.09)\n",
        "epochs = 10\n",
        "\n",
        "lossv, accv = [], []\n",
        "start=time.time()\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train(epoch)\n",
        "    test(lossv, accv)\n",
        "end=time.time()\n",
        "df_cm = pd.DataFrame(confusion_matrix, index = [i for i in class_name],\n",
        "                  columns = [i for i in class_name])\n",
        "\n",
        "#plot confusion matrix for this network \n",
        "plt.figure(figsize = (20,10))\n",
        "x=sn.heatmap(df_cm, annot=True)\n",
        "\n",
        "arr.append(round(float(accv[-1]),2))\n",
        "arr.append(round(float(end-start),2))\n",
        "array.append(arr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvFb41eCB_26",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Adadelta optimizer with L2 regularization\n",
        "arr=['Adadelta with L2 Regularization']\n",
        "\n",
        "#creating confusion matrix\n",
        "confusion_matrix=np.array([[0 for x in range(10)] for y in range(10)])\n",
        "confusion_matrix.astype(int)\n",
        "\n",
        "#L2 regularizer giver as parameters to Adadelta using weight_decay\n",
        "optimizer  = optim.Adadelta(model_l2.parameters(), lr=0.001, weight_decay=0.09)\n",
        "epochs = 10\n",
        "\n",
        "lossv, accv = [], []\n",
        "start=time.time()\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train(epoch)\n",
        "    test(lossv, accv)\n",
        "end=time.time()\n",
        "df_cm = pd.DataFrame(confusion_matrix, index = [i for i in class_name],\n",
        "                  columns = [i for i in class_name])\n",
        "\n",
        "#plot confusion matrix for this network \n",
        "plt.figure(figsize = (20,10))\n",
        "x=sn.heatmap(df_cm, annot=True)\n",
        "\n",
        "arr.append(round(float(accv[-1]),2))\n",
        "arr.append(round(float(end-start),2))\n",
        "array.append(arr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcj2Yz79E-_y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Adam optimizer with L2 regularization\n",
        "arr=['Adam with L2 Regularization']\n",
        "\n",
        "#creating confusion matrix\n",
        "confusion_matrix=np.array([[0 for x in range(10)] for y in range(10)])\n",
        "confusion_matrix.astype(int)\n",
        "\n",
        "#L2 regularizer giver as parameters to Adam using weight_decay\n",
        "optimizer = optim.Adam(model_l2.parameters(), lr=0.001, eps=1e-08, weight_decay=0.09, amsgrad=False)\n",
        "epochs = 10\n",
        "\n",
        "lossv, accv = [], []\n",
        "start=time.time()\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train(epoch)\n",
        "    test(lossv, accv)\n",
        "end=time.time()\n",
        "df_cm = pd.DataFrame(confusion_matrix, index = [i for i in class_name],\n",
        "                  columns = [i for i in class_name])\n",
        "\n",
        "#plot confusion matrix for this network \n",
        "plt.figure(figsize = (20,10))\n",
        "x=sn.heatmap(df_cm, annot=True)\n",
        "\n",
        "arr.append(round(float(accv[-1]),2))\n",
        "arr.append(round(float(end-start),2))\n",
        "array.append(arr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5rqbgTyE-yV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Adagrad optimizer with L2 regularization\n",
        "arr=['Adagrad with L2 Regularization']\n",
        "\n",
        "#creating confusion matrix\n",
        "confusion_matrix=np.array([[0 for x in range(10)] for y in range(10)])\n",
        "confusion_matrix.astype(int)\n",
        "\n",
        "#L2 regularizer giver as parameters to Adagrad using weight_decay\n",
        "optimizer = optim.Adagrad(model_l2.parameters(), lr=0.01, lr_decay=0.001, weight_decay=0.09, initial_accumulator_value=0)\n",
        "epochs = 10\n",
        "\n",
        "lossv, accv = [], []\n",
        "start=time.time()\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train(epoch)\n",
        "    test(lossv, accv)\n",
        "end=time.time()\n",
        "df_cm = pd.DataFrame(confusion_matrix, index = [i for i in class_name],\n",
        "                  columns = [i for i in class_name])\n",
        "\n",
        "#plot confusion matrix for this network \n",
        "plt.figure(figsize = (20,10))\n",
        "x=sn.heatmap(df_cm, annot=True)\n",
        "\n",
        "arr.append(round(float(accv[-1]),2))\n",
        "arr.append(round(float(end-start),2))\n",
        "array.append(arr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9Nz6L1dE-qP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#RMSProp optimizer with L2 regularization\n",
        "arr=['RMSProp with L2 Regularization']\n",
        "\n",
        "#creating confusion matrix\n",
        "confusion_matrix=np.array([[0 for x in range(10)] for y in range(10)])\n",
        "confusion_matrix.astype(int)\n",
        "\n",
        "#L2 regularizer giver as parameters to Adagrad using weight_decay\n",
        "optimizer = optim.RMSprop(model_l2.parameters(), lr=0.01, alpha=0.99, eps=1e-08, weight_decay=0.09, momentum=0, centered=False)\n",
        "epochs = 10\n",
        "\n",
        "lossv, accv = [], []\n",
        "start=time.time()\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train(epoch)\n",
        "    test(lossv, accv)\n",
        "end=time.time()\n",
        "df_cm = pd.DataFrame(confusion_matrix, index = [i for i in class_name],\n",
        "                  columns = [i for i in class_name])\n",
        "\n",
        "\n",
        "#plot confusion matrix for this network \n",
        "plt.figure(figsize = (20,10))\n",
        "x=sn.heatmap(df_cm, annot=True)\n",
        "\n",
        "arr.append(round(float(accv[-1]),2))\n",
        "arr.append(round(float(end-start),2))\n",
        "array.append(arr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_p8_n6rE-h1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Network using L1 regularization with relu, tanh and sigmoid with 3 layers\n",
        "class Net_L1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net_L1, self).__init__()\n",
        "        self.fc1 = nn.Linear(3*32*32,70)\n",
        "        self.fc2 = nn.Linear(70, 50)\n",
        "        self.fc3 = nn.Linear(50, 10)\n",
        "        \n",
        "        \n",
        "       \n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 3*32*32)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = torch.tanh(self.fc2(x))\n",
        "        return torch.sigmoid(self.fc3(x))\n",
        "        \n",
        "        \n",
        "\n",
        "model_l1 = Net_L1()\n",
        "if cuda:\n",
        "    model_l1.cuda()\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "print(model_l1)\n",
        "\n",
        "#Training Dataset with L1 regularization\n",
        "\n",
        "def train_l1(epoch, log_interval=1000):\n",
        "    model_l1.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        if cuda:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        output = model_l1(data)\n",
        "        l1_reg_lamda=0.5\n",
        "        l1_reg=0\n",
        "        \n",
        "        for W in model_l1.parameters():\n",
        "          W=Variable(W,requires_grad=True)\n",
        "          l1_reg += torch.sum(torch.abs(W))\n",
        "        \n",
        "        \n",
        "        loss = F.nll_loss(output, target) + l1_reg_lamda*l1_reg\n",
        "        loss.backward()\n",
        "        \n",
        "        W.grad.data\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * float(batch_idx) / len(train_loader), loss.item()))\n",
        "            \n",
        "            \n",
        "def test_l1(loss_vector, accuracy_vector):\n",
        "    model_l1.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    for data, target in test_loader:\n",
        "        if cuda:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        data.requires_grad_(False)\n",
        "        target.requires_grad_(False)\n",
        "        output = model_l1(data)\n",
        "        test_loss += F.nll_loss(output, target)\n",
        "        \n",
        "        pred = output.data.max(1)[1]\n",
        "        test_loss += F.nll_loss(output, target) \n",
        "        correct += pred.eq(target.data).cpu().sum().float()\n",
        "        #Updating confusion matrix\n",
        "        for i in range(50):\n",
        "                  confusion_matrix[pred[i]][target[i]]+=1\n",
        "       \n",
        "    test_loss /= len(test_loader)\n",
        "    loss_vector.append(test_loss)\n",
        "    \n",
        "    accuracy = 100. * correct / float(len(test_loader.dataset))\n",
        "    accuracy_vector.append(accuracy)\n",
        "    \n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset), float(accuracy)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PANYaDoJFO_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#SGD optimizer with L1 regularization\n",
        "arr=['SGD with L1 Regularization']\n",
        "\n",
        "#creating confusion matrix\n",
        "confusion_matrix=np.array([[0 for x in range(10)] for y in range(10)])\n",
        "confusion_matrix.astype(int)\n",
        "\n",
        "#L1 regularizer giver as parameters to SGD using weight_decay\n",
        "optimizer = optim.SGD(model_l1.parameters(), lr=0.001, momentum=0.5)\n",
        "epochs = 10\n",
        "\n",
        "lossv, accv = [], []\n",
        "start=time.time()\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train(epoch)\n",
        "    test(lossv, accv)\n",
        "end=time.time()\n",
        "df_cm = pd.DataFrame(confusion_matrix, index = [i for i in class_name],\n",
        "                  columns = [i for i in class_name])\n",
        "\n",
        "\n",
        "#plot confusion matrix for this network \n",
        "plt.figure(figsize = (20,10))\n",
        "x=sn.heatmap(df_cm, annot=True)\n",
        "\n",
        "arr.append(round(float(accv[-1]),2))\n",
        "arr.append(round(float(end-start),2))\n",
        "array.append(arr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAAnrTFJJkDn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#adadelta optimizer with L1 regularization\n",
        "arr=['adadelta with L1 Regularization']\n",
        "\n",
        "#creating confusion matrix\n",
        "confusion_matrix=np.array([[0 for x in range(10)] for y in range(10)])\n",
        "confusion_matrix.astype(int)\n",
        "\n",
        "#L1 regularizer giver as parameters to adadelta using weight_decay\n",
        "optimizer  = optim.Adadelta(model_l1.parameters(), lr=0.001, weight_decay=0.09)\n",
        "epochs = 10\n",
        "\n",
        "lossv, accv = [], []\n",
        "start=time.time()\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train(epoch)\n",
        "    test(lossv, accv)\n",
        "end=time.time()\n",
        "df_cm = pd.DataFrame(confusion_matrix, index = [i for i in class_name],\n",
        "                  columns = [i for i in class_name])\n",
        "\n",
        "#plot confusion matrix for this network \n",
        "plt.figure(figsize = (20,10))\n",
        "x=sn.heatmap(df_cm, annot=True)\n",
        "\n",
        "arr.append(round(float(accv[-1]),2))\n",
        "arr.append(round(float(end-start),2))\n",
        "array.append(arr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhgT1cCXJj99",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#adam optimizer with L1 regularization\n",
        "arr=['adam with L1 Regularization']\n",
        "\n",
        "#creating confusion matrix\n",
        "confusion_matrix=np.array([[0 for x in range(10)] for y in range(10)])\n",
        "confusion_matrix.astype(int)\n",
        "\n",
        "#L1 regularizer giver as parameters to adaM using weight_decay\n",
        "optimizer = optim.Adam(model_l1.parameters(), lr=0.001, eps=1e-08, weight_decay=0.09, amsgrad=False)\n",
        "epochs = 10\n",
        "\n",
        "lossv, accv = [], []\n",
        "start=time.time()\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train(epoch)\n",
        "    test(lossv, accv)\n",
        "end=time.time()\n",
        "df_cm = pd.DataFrame(confusion_matrix, index = [i for i in class_name],\n",
        "                  columns = [i for i in class_name])\n",
        "\n",
        "#plot confusion matrix for this network \n",
        "plt.figure(figsize = (20,10))\n",
        "x=sn.heatmap(df_cm, annot=True)\n",
        "\n",
        "arr.append(round(float(accv[-1]),2))\n",
        "arr.append(round(float(end-start),2))\n",
        "array.append(arr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgi8JXWNJj4u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#adagrad optimizer with L1 regularization\n",
        "arr=['adagrad with L1 Regularization']\n",
        "\n",
        "#creating confusion matrix\n",
        "confusion_matrix=np.array([[0 for x in range(10)] for y in range(10)])\n",
        "confusion_matrix.astype(int)\n",
        "\n",
        "#L1 regularizer giver as parameters to adagrad using weight_decay\n",
        "optimizer = optim.Adagrad(model_l1.parameters(), lr=0.01, lr_decay=0.001, weight_decay=0.09, initial_accumulator_value=0)\n",
        "epochs = 10\n",
        "\n",
        "lossv, accv = [], []\n",
        "start=time.time()\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train(epoch)\n",
        "    test(lossv, accv)\n",
        "end=time.time()\n",
        "df_cm = pd.DataFrame(confusion_matrix, index = [i for i in class_name],\n",
        "                  columns = [i for i in class_name])\n",
        "\n",
        "#plot confusion matrix for this network \n",
        "plt.figure(figsize = (20,10))\n",
        "x=sn.heatmap(df_cm, annot=True)\n",
        "\n",
        "arr.append(round(float(accv[-1]),2))\n",
        "arr.append(round(float(end-start),2))\n",
        "array.append(arr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iRE8ES9Jj0V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#RMSprop optimizer with L1 regularization\n",
        "arr=['RMSprop with L1 Regularization']\n",
        "\n",
        "#creating confusion matrix\n",
        "confusion_matrix=np.array([[0 for x in range(10)] for y in range(10)])\n",
        "confusion_matrix.astype(int)\n",
        "\n",
        "#L1 regularizer giver as parameters to RMSprop using weight_decay\n",
        "optimizer = optim.RMSprop(model_l1.parameters(), lr=0.01, alpha=0.99, eps=1e-08, weight_decay=0.09, momentum=0, centered=False)\n",
        "epochs = 10\n",
        "\n",
        "lossv, accv = [], []\n",
        "start=time.time()\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train(epoch)\n",
        "    test(lossv, accv)\n",
        "end=time.time()\n",
        "df_cm = pd.DataFrame(confusion_matrix, index = [i for i in class_name],\n",
        "                  columns = [i for i in class_name])\n",
        "\n",
        "#plot confusion matrix for this network\n",
        "plt.figure(figsize = (20,10))\n",
        "x=sn.heatmap(df_cm, annot=True)\n",
        "\n",
        "arr.append(round(float(accv[-1]),2))\n",
        "arr.append(round(float(end-start),2))\n",
        "array.append(arr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rdbv26rWKkqG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Table comparing different optimizers details of accuracy and convergence time\n",
        "\n",
        "print(tabulate(array,\n",
        "       headers=\"firstrow\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wy9H3uHd_uQa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Network to test defined hinge loss function\n",
        "class Net_Hinge(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net_Hinge, self).__init__()\n",
        "        self.fc1 = nn.Linear(3*32*32, 2000)\n",
        "        self.fc1_drop = nn.Dropout(0.2)\n",
        "        self.fc2 = nn.Linear(2000, 200)\n",
        "        self.fc2_drop = nn.Dropout(0.2)\n",
        "        self.fc3 = nn.Linear(200, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 3*32*32)\n",
        "        x = torch.sigmoid(self.fc1(x))\n",
        "        x = self.fc1_drop(x)\n",
        "        x = torch.sigmoid(self.fc2(x))\n",
        "        x = self.fc2_drop(x)\n",
        "        return F.log_softmax(self.fc3(x),dim=1)\n",
        "\n",
        "model = Net_Hinge()\n",
        "if cuda:\n",
        "    model.cuda()\n",
        "    \n",
        "def hingeloss(input,target):\n",
        "  hinge_loss=0\n",
        "  for i in range(len(input)):\n",
        "    sy=input[i][target[i]]\n",
        "    ay=[si-sy+1 for si in input[i]]\n",
        "    ay=np.max(ay)\n",
        "    if(ay>0):\n",
        "      hinge_loss+=ay\n",
        "  return hinge_loss\n",
        "\n",
        "def train_hinge(epoch, log_interval=100):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        if cuda:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = hingeloss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * float(batch_idx) / len(train_loader), loss.item()))\n",
        "\n",
        "def test_hinge(loss_vector, accuracy_vector):\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    for data, target in test_loader:\n",
        "        if cuda:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        output = model(data)\n",
        "        test_loss += hingeloss(output, target).item()\n",
        "        pred = output.data.max(1)[1] \n",
        "        correct += pred.eq(target.data).cpu().sum().float()\n",
        "        for i in range(50):\n",
        "                  confusion_matrix[pred[i]][target[i]]+=1\n",
        "    test_loss /= len(test_loader)\n",
        "    loss_vector.append(test_loss)\n",
        "    \n",
        "    accuracy = 100. * correct / float(len(test_loader.dataset))\n",
        "    accuracy_vector.append(accuracy)\n",
        "    \n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset), float(accuracy)))\n",
        "\n",
        "    \n",
        "#Running network with defined hinge loss function\n",
        "arr=['SGD with Dropout']\n",
        "confusion_matrix=np.array([[0 for x in range(10)] for y in range(10)])\n",
        "confusion_matrix.astype(int)\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
        "epochs = 10\n",
        "\n",
        "lossv, accv = [], []\n",
        "start=time.time()\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train_hinge(epoch)\n",
        "    test_hinge(lossv, accv)\n",
        "end=time.time()\n",
        "df_cm = pd.DataFrame(confusion_matrix, index = [i for i in class_name],\n",
        "                  columns = [i for i in class_name])\n",
        "\n",
        "plt.figure(figsize = (20,10))\n",
        "x=sn.heatmap(df_cm, annot=True)\n",
        "\n",
        "arr.append(round(float(accv[-1]),2))\n",
        "arr.append(round(float(end-start),2))\n",
        "array.append(arr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rygjBAGNAydp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#finding the best depth-Depth given now is 1 hidden layer\n",
        "#Network initiation\n",
        "class Net_depth_1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net_depth_1, self).__init__()\n",
        "        self.fc1 = nn.Linear(3*32*32, 2000)\n",
        "        self.fc1_drop = nn.Dropout(0.2)\n",
        "        self.fc2 = nn.Linear(2000, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 3*32*32)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc1_drop(x)\n",
        "        return F.log_softmax(self.fc2(x),dim=1)\n",
        "    \n",
        "\n",
        "model = Net_depth_1()\n",
        "if cuda:\n",
        "    model.cuda()\n",
        "    \n",
        "print(model)\n",
        "\n",
        "\n",
        "#SGD optimizer with Dropout regularization\n",
        "arr=['1 Layer']\n",
        "\n",
        "#creating confusion matrix\n",
        "confusion_matrix=np.array([[0 for x in range(10)] for y in range(10)])\n",
        "confusion_matrix.astype(int)\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
        "epochs = 10\n",
        "\n",
        "lossv, accv = [], []\n",
        "start=time.time()\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train(epoch)\n",
        "    test(lossv, accv)\n",
        "end=time.time()\n",
        "df_cm = pd.DataFrame(confusion_matrix, index = [i for i in class_name],\n",
        "                  columns = [i for i in class_name])\n",
        "\n",
        "for col in class_name:\n",
        "   df_cm[col] = df_cm[col].apply(lambda x: int(x) if x == x else \"\")\n",
        "plt.figure(figsize = (20,10))\n",
        "x=sn.heatmap(df_cm, annot=True)\n",
        "\n",
        "arr.append(round(float(accv[-1]),2))\n",
        "arr.append(round(float(end-start),2))\n",
        "depth_array.append(arr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWcPLUYyC7vB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#finding the best depth-Depth given now is 2 hidden layers\n",
        "#Network initiation\n",
        "class Net_depth_2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net_depth_2, self).__init__()\n",
        "        self.fc1 = nn.Linear(3*32*32, 2000)\n",
        "        self.fc1_drop = nn.Dropout(0.2)\n",
        "        self.fc2 = nn.Linear(2000, 200)\n",
        "        self.fc2_drop = nn.Dropout(0.2)\n",
        "        self.fc3 = nn.Linear(200, 10)\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 3*32*32)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc1_drop(x)\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc2_drop(x)\n",
        "        return F.log_softmax(self.fc3(x),dim=1)\n",
        "    \n",
        "\n",
        "model = Net_depth_2()\n",
        "if cuda:\n",
        "    model.cuda()\n",
        "    \n",
        "print(model)   \n",
        "\n",
        "\n",
        "#SGD optimizer with Dropout regularization\n",
        "arr=['2 Layers']\n",
        "\n",
        "#creating confusion matrix\n",
        "confusion_matrix=np.array([[0 for x in range(10)] for y in range(10)])\n",
        "confusion_matrix.astype(int)\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
        "epochs = 10\n",
        "\n",
        "lossv, accv = [], []\n",
        "start=time.time()\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train(epoch)\n",
        "    test(lossv, accv)\n",
        "end=time.time()\n",
        "df_cm = pd.DataFrame(confusion_matrix, index = [i for i in class_name],\n",
        "                  columns = [i for i in class_name])\n",
        "\n",
        "for col in class_name:\n",
        "   df_cm[col] = df_cm[col].apply(lambda x: int(x) if x == x else \"\")\n",
        "plt.figure(figsize = (20,10))\n",
        "x=sn.heatmap(df_cm, annot=True)\n",
        "\n",
        "arr.append(round(float(accv[-1]),2))\n",
        "arr.append(round(float(end-start),2))\n",
        "depth_array.append(arr)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSU1YG5hDbz4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#finding the best depth-Depth given now is 3 hidden layers\n",
        "#Network initiation\n",
        "class Net_depth_3(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net_depth_3, self).__init__()\n",
        "        self.fc1 = nn.Linear(3*32*32, 2000)\n",
        "        self.fc1_drop = nn.Dropout(0.2)\n",
        "        self.fc2 = nn.Linear(2000, 1000)\n",
        "        self.fc2_drop = nn.Dropout(0.2)\n",
        "        self.fc3 = nn.Linear(1000, 200)\n",
        "        self.fc3_drop = nn.Dropout(0.2)\n",
        "        self.fc4 = nn.Linear(200, 10)\n",
        "        \n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 3*32*32)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc1_drop(x)\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc2_drop(x)\n",
        "        x = torch.relu(self.fc3(x))\n",
        "        x = self.fc3_drop(x)\n",
        "        return F.log_softmax(self.fc4(x),dim=1)\n",
        "    \n",
        "\n",
        "model = Net_depth_3()\n",
        "if cuda:\n",
        "    model.cuda()\n",
        "    \n",
        "    \n",
        "print(model)\n",
        "\n",
        "\n",
        "#SGD optimizer with Dropout regularization\n",
        "arr=['3 Layers']\n",
        "\n",
        "#creating confusion matrix\n",
        "confusion_matrix=np.array([[0 for x in range(10)] for y in range(10)])\n",
        "confusion_matrix.astype(int)\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
        "epochs = 10\n",
        "\n",
        "lossv, accv = [], []\n",
        "start=time.time()\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train(epoch)\n",
        "    test(lossv, accv)\n",
        "end=time.time()\n",
        "df_cm = pd.DataFrame(confusion_matrix, index = [i for i in class_name],\n",
        "                  columns = [i for i in class_name])\n",
        "\n",
        "for col in class_name:\n",
        "   df_cm[col] = df_cm[col].apply(lambda x: int(x) if x == x else \"\")\n",
        "plt.figure(figsize = (20,10))\n",
        "x=sn.heatmap(df_cm, annot=True)\n",
        "\n",
        "arr.append(round(float(accv[-1]),2))\n",
        "arr.append(round(float(end-start),2))\n",
        "depth_array.append(arr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v16JUWOGDqLb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#finding the best depth-Depth given now is 4 hidden layers\n",
        "#Network initiation\n",
        "class Net_depth_4(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net_depth_4, self).__init__()\n",
        "        self.fc1 = nn.Linear(3*32*32, 2000)\n",
        "        self.fc1_drop = nn.Dropout(0.2)\n",
        "        self.fc2 = nn.Linear(2000, 1000)\n",
        "        self.fc2_drop = nn.Dropout(0.2)\n",
        "        self.fc3 = nn.Linear(1000, 200)\n",
        "        self.fc3_drop = nn.Dropout(0.2)\n",
        "        self.fc4 = nn.Linear(200, 100)\n",
        "        self.fc4_drop = nn.Dropout(0.2)\n",
        "        self.fc5 = nn.Linear(100, 10)\n",
        "        \n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 3*32*32)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc1_drop(x)\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc2_drop(x)\n",
        "        x = torch.relu(self.fc3(x))\n",
        "        x = self.fc3_drop(x)\n",
        "        x = torch.relu(self.fc4(x))\n",
        "        x = self.fc4_drop(x)\n",
        "        return F.log_softmax(self.fc5(x),dim=1)\n",
        "    \n",
        "\n",
        "model = Net_depth_4()\n",
        "if cuda:\n",
        "    model.cuda()\n",
        "    \n",
        "    \n",
        "print(model)\n",
        "\n",
        "#SGD optimizer with Dropout regularization\n",
        "arr=['4 Layers']\n",
        "\n",
        "#creating confusion matrix\n",
        "confusion_matrix=np.array([[0 for x in range(10)] for y in range(10)])\n",
        "confusion_matrix.astype(int)\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
        "epochs = 10\n",
        "\n",
        "lossv, accv = [], []\n",
        "start=time.time()\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train(epoch)\n",
        "    test(lossv, accv)\n",
        "end=time.time()\n",
        "df_cm = pd.DataFrame(confusion_matrix, index = [i for i in class_name],\n",
        "                  columns = [i for i in class_name])\n",
        "\n",
        "for col in class_name:\n",
        "   df_cm[col] = df_cm[col].apply(lambda x: int(x) if x == x else \"\")\n",
        "plt.figure(figsize = (20,10))\n",
        "x=sn.heatmap(df_cm, annot=True)\n",
        "\n",
        "arr.append(round(float(accv[-1]),2))\n",
        "arr.append(round(float(end-start),2))\n",
        "depth_array.append(arr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgQ2tbM3DuLy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#finding the best depth-Depth given now is 5 hidden layers\n",
        "#Network initiation\n",
        "class Net_depth_5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net_depth_5, self).__init__()\n",
        "        self.fc1 = nn.Linear(3*32*32, 2000)\n",
        "        self.fc1_drop = nn.Dropout(0.2)\n",
        "        self.fc2 = nn.Linear(2000, 1000)\n",
        "        self.fc2_drop = nn.Dropout(0.2)\n",
        "        self.fc3 = nn.Linear(1000, 200)\n",
        "        self.fc3_drop = nn.Dropout(0.2)\n",
        "        self.fc4 = nn.Linear(200, 100)\n",
        "        self.fc4_drop = nn.Dropout(0.2)\n",
        "        self.fc5 = nn.Linear(100, 50)\n",
        "        self.fc5_drop = nn.Dropout(0.2)\n",
        "        self.fc6 = nn.Linear(50, 10)\n",
        "        \n",
        "        \n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 3*32*32)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc1_drop(x)\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc2_drop(x)\n",
        "        x = torch.relu(self.fc3(x))\n",
        "        x = self.fc3_drop(x)\n",
        "        x = torch.relu(self.fc4(x))\n",
        "        x = self.fc4_drop(x)\n",
        "        x = torch.relu(self.fc5(x))\n",
        "        x = self.fc5_drop(x)\n",
        "        return F.log_softmax(self.fc6(x),dim=1)\n",
        "    \n",
        "\n",
        "model = Net_depth_5()\n",
        "if cuda:\n",
        "    model.cuda()\n",
        "    \n",
        "    \n",
        "print(model)\n",
        "\n",
        "#SGD optimizer with Dropout regularization\n",
        "arr=['5 Layers']\n",
        "\n",
        "#creating confusion matrix\n",
        "confusion_matrix=np.array([[0 for x in range(10)] for y in range(10)])\n",
        "confusion_matrix.astype(int)\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
        "epochs = 10\n",
        "\n",
        "lossv, accv = [], []\n",
        "start=time.time()\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train(epoch)\n",
        "    test(lossv, accv)\n",
        "end=time.time()\n",
        "df_cm = pd.DataFrame(confusion_matrix, index = [i for i in class_name],\n",
        "                  columns = [i for i in class_name])\n",
        "\n",
        "for col in class_name:\n",
        "   df_cm[col] = df_cm[col].apply(lambda x: int(x) if x == x else \"\")\n",
        "plt.figure(figsize = (20,10))\n",
        "x=sn.heatmap(df_cm, annot=True)\n",
        "\n",
        "arr.append(round(float(accv[-1]),2))\n",
        "arr.append(round(float(end-start),2))\n",
        "depth_array.append(arr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEBzOjRsDx9o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(tabulate(depth_array,\n",
        "       headers=\"firstrow\"))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}